{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learn MNIST data set.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FCZBMVPPU5na"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t2wain/colab/blob/master/Learn_MNIST_data_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oVfh3gDPQg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZBMVPPU5na",
        "colab_type": "text"
      },
      "source": [
        "# Inspecting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkjcgVWmQMiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "digits = datasets.load_digits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAgiAhWIRtwy",
        "colab_type": "code",
        "outputId": "66a9c668-0106-4250-c38f-f38ecac4b8f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(\"data:\", digits.data.shape, type(digits.data))\n",
        "print(\"target:\", digits.target.shape, type(digits.target))\n",
        "print(\"target_names:\", digits.target_names.shape, type(digits.target_names))\n",
        "print(\"images:\", digits.images.shape, type(digits.images))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: (1797, 64) <class 'numpy.ndarray'>\n",
            "target: (1797,) <class 'numpy.ndarray'>\n",
            "target_names: (10,) <class 'numpy.ndarray'>\n",
            "images: (1797, 8, 8) <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9BWnNASTpYh",
        "colab_type": "code",
        "outputId": "4be5fae4-d1ff-4a72-a88e-2ba3c9f4e3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
        "    plt.subplot(2, 5, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('Training: %i' % label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADfCAYAAADWQznrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW5IKY3BLUmEMbkkqjMEtSYX5f+JS8Lp4AVjOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97UWz8OV3vb",
        "colab_type": "text"
      },
      "source": [
        "# Example 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6ljKYDV65D",
        "colab_type": "text"
      },
      "source": [
        "## Utility methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zm1II56Y3Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(X, Y=None, batch_size=1, epochs=1):\n",
        "  X_size, _ = X.shape\n",
        "\n",
        "  for step in range(epochs):\n",
        "    for step in range(X_size // batch_size):\n",
        "      offset = (step * batch_size) % X_size\n",
        "      batch_x = X[offset:(offset + batch_size), :]\n",
        "      batch_y = None if Y is None else Y[offset:(offset + batch_size)]\n",
        "      yield (batch_x, batch_y)\n",
        "    if X_size % batch_size > 0:\n",
        "      step += 1\n",
        "      offset = (step * batch_size) % X_size\n",
        "      batch_x = X[offset:, :]\n",
        "      batch_y = None if Y is None else Y[offset:]\n",
        "      yield (batch_x, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4m55J7ln6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dnn(num_features, num_labels, hiddens=[]):\n",
        "  \n",
        "  def loss(Y, y_prob):\n",
        "    xentropy = -tf.reduce_sum(Y * tf.log(y_prob), reduction_indices=1)\n",
        "    loss = tf.reduce_mean(xentropy, name='cost')\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def loss_logit(Y, logit):\n",
        "    xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)\n",
        "    loss = tf.reduce_mean(tf.reduce_sum(xentropy), name='cost')\n",
        "    #loss = tf.reduce_sum(xentropy, name='cost')\n",
        "    return loss\n",
        "\n",
        "\n",
        "  num_sample = None\n",
        "  with tf.name_scope('placeholders'):\n",
        "    Xin = tf.placeholder(tf.float32, shape=[num_sample, num_features], name='Xin')\n",
        "    Yin = tf.placeholder(tf.float32, shape=[num_sample, num_labels], name='Yin')\n",
        "    rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
        "  \n",
        "  X = Xin; W = None; layers = []; y_out = None\n",
        "  # create nodes for hidden layers\n",
        "  num_input = num_features\n",
        "  num_hidden_layers = len(hiddens)\n",
        "\n",
        "  for layer_num, num_node_output in enumerate(hiddens):\n",
        "    layer_name = \"hidden_layer_%d\" % (layer_num+1)\n",
        "    with tf.name_scope(layer_name):\n",
        "      with tf.variable_scope(layer_name):\n",
        "        W = tf.get_variable('W', \n",
        "          initializer=tf.truncated_normal([num_input, num_node_output], stddev=0.1))\n",
        "        b = tf.get_variable('b', \n",
        "          initializer=tf.Variable(tf.constant(0.1,shape=[num_node_output])))\n",
        "      fx = tf.add(tf.matmul(X, W), b) # linear regression\n",
        "      y_out = tf.nn.relu6(fx) # activation funtion to introduce non-linearity\n",
        "      # apply dropout to hidden layer to introduce regularization of W\n",
        "      layer_drop = tf.nn.dropout(y_out, rate=rate, name='y_out')\n",
        "      y_out = layer_drop\n",
        "      # keeping track of each layer\n",
        "      layers.append({\"W\": W, \"b\": b, \"out\": y_out, \"W_val\": None, \"b_val\": None})\n",
        "      X = y_out # y become Xin to next layer\n",
        "      num_input = num_node_output\n",
        "  \n",
        "  # create output layer\n",
        "  num_input = num_features if W is None else num_input\n",
        "  num_node_output = num_labels\n",
        "  with tf.name_scope('output'):\n",
        "    with tf.variable_scope('output'):\n",
        "      W = tf.get_variable('W', \n",
        "        initializer=tf.truncated_normal([num_input, num_node_output], stddev=0.1))\n",
        "      b = tf.get_variable('b', \n",
        "        initializer=tf.constant(0.1,shape=[num_node_output]))\n",
        "    y_logit = tf.add(tf.matmul(X, W), b) # logistic regression, ln-odd\n",
        "    # convert to probability, exp-logit, and normalize to interval between 0 and 1\n",
        "    y_prob = tf.nn.softmax(y_logit, name='y_prob')\n",
        "    layers.append({\"W\": W, \"b\": b, \"out\": y_prob, \"W_val\": None, \"b_val\": None})\n",
        "    y_pred = tf.argmax(y_prob, 1, name='y_pred') # index of max probablity\n",
        "    y_label = tf.argmax(Yin, 1, name='y_label') # index of one-hot label\n",
        "\n",
        "  with tf.name_scope('optimize'):\n",
        "    cost = loss_logit(Yin, y_logit)\n",
        "    #cost = loss(Yin, y_prob)\n",
        "    op = tf.train.AdamOptimizer()\n",
        "    # minimize cost/loss\n",
        "    train_op = op.minimize(cost)\n",
        "  \n",
        "  with tf.name_scope('metrics'):\n",
        "    correct_prediction = tf.equal(y_pred, y_label)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name='accuracy')\n",
        "  \n",
        "  return {\"x\": Xin, \"y\": Yin, \"dropout_rate\": rate, \"layers\": layers, \n",
        "          \"cost\": cost, \"accuracy\": accuracy, \"y_prob\": y_prob, \"y_pred\": y_pred,\n",
        "          \"op\": op, \"train_op\": train_op}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-Zi4LkjF_Gs",
        "colab": {}
      },
      "source": [
        "def build_classifier_estimator(num_features, num_labels, hiddens=[], msg_per_steps=0):\n",
        "  is_saved = False\n",
        "  \n",
        "  g1 = tf.Graph()\n",
        "  with g1.as_default() as graph:\n",
        "    dnn = build_dnn(num_features, num_labels, hiddens)\n",
        "\n",
        "  # placeholders, input to the training\n",
        "  X = dnn[\"x\"]\n",
        "  Y = dnn[\"y\"]\n",
        "  rate = dnn[\"dropout_rate\"]\n",
        "\n",
        "  # optimizer\n",
        "  op = dnn[\"op\"]\n",
        "  train_op = dnn[\"train_op\"]\n",
        "\n",
        "  # metrics\n",
        "  cost = dnn[\"cost\"]\n",
        "  accuracy = dnn[\"accuracy\"]\n",
        "  y_pred = dnn[\"y_pred\"]\n",
        "  y_prob = dnn[\"y_prob\"]\n",
        "  \n",
        "\n",
        "  def train(xsample, ysample,  batch_size=1, training_epochs=1, \n",
        "            learning_rate=0.001, dropout_rate=0, max_loss=0.01, limit_count=4):\n",
        "    op.learning_rate = learning_rate\n",
        "    loss_count = 0\n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      step_iterator = enumerate(data_gen(xsample, ysample,  batch_size, training_epochs))\n",
        "      for step, (batch_xs, batch_labels) in step_iterator:\n",
        "        feed_dict = {X: batch_xs, Y: batch_labels, rate: dropout_rate}\n",
        "        sess.run(train_op, feed_dict=feed_dict)\n",
        "\n",
        "        # print training progress\n",
        "        if msg_per_steps > 0 and step % msg_per_steps == 0:\n",
        "          feed_dict = {X: batch_xs, Y: batch_labels, rate: 0}\n",
        "          cost_val, accuracy_val = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
        "          print(\n",
        "              \"Iteration\", str(step), \n",
        "              \"\\t| Loss =\", str(cost_val), \n",
        "              \"\\t| Accuracy =\", str(accuracy_val))\n",
        "          \n",
        "        # early training stop\n",
        "        if math.isnan(cost_val) or loss_count > limit_count:\n",
        "          break;\n",
        "        elif cost_val < max_loss:\n",
        "          loss_count += 1\n",
        "          \n",
        "      _saveVars(sess)\n",
        "\n",
        "      \n",
        "  def _restoreVars(sess):\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    if is_saved:\n",
        "      for layer in dnn[\"layers\"]:\n",
        "        sess.run(layer[\"W\"].assign(layer[\"W_val\"]))\n",
        "        sess.run(layer[\"b\"].assign(layer[\"b_val\"]))\n",
        "\n",
        "    \n",
        "  def _saveVars(sess):\n",
        "    nonlocal is_saved\n",
        "    is_saved = True\n",
        "    for layer in dnn[\"layers\"]:\n",
        "      layer[\"W_val\"] = sess.run(layer[\"W\"])\n",
        "      layer[\"b_val\"] = sess.run(layer[\"b\"])\n",
        "\n",
        "      \n",
        "  def evaluate(xsample, ysample, batch_size=1):\n",
        "    if not is_saved:\n",
        "      return \"Error: Training has not been done.\"\n",
        "\n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      cost_total = accuracy_total = step = 0\n",
        "      for step, (batch_xs, batch_ys) in enumerate(data_gen(xsample, ysample,  batch_size)):\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, rate: 0}\n",
        "        cost_val, accuracy_val = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
        "        cost_total += cost_val\n",
        "        accuracy_total += accuracy_val\n",
        "      return (cost_val/(step+1), accuracy_total/(step+1))\n",
        "\n",
        "    \n",
        "  def predict(xdata, labels, batch_size=1):\n",
        "    if not is_saved:\n",
        "      return \"Error: Training has not been done.\"\n",
        "      \n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      pred = []; prob = []\n",
        "      for (batch_xs, _) in data_gen(xdata, batch_size=batch_size):\n",
        "        feed_dict={X: batch_xs, rate: 0}\n",
        "        ypred = sess.run(y_pred, feed_dict=feed_dict)\n",
        "        yprob = sess.run(y_prob, feed_dict=feed_dict)\n",
        "        pred.extend(ypred)\n",
        "        prob.extend(yprob.round(3))\n",
        "        \n",
        "      df = pd.DataFrame(prob, columns=labels)\n",
        "      plabel = labels[pred]\n",
        "      df[\"ypred\"] = plabel\n",
        "      return df\n",
        "\n",
        "    \n",
        "  return (train, evaluate, predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAS37b0NfoIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(labels, ydata):\n",
        "  y_target = np.expand_dims(ydata, axis=1)\n",
        "  enc = OneHotEncoder(categories=[labels])\n",
        "  enc.fit(y_target)\n",
        "  return (labels, enc.transform(y_target).toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdTWTZuAWbAP",
        "colab_type": "text"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWHbAea6ns4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tensors_ex():\n",
        "  g1 = tf.Graph()\n",
        "  with g1.as_default() as graph:\n",
        "    dnn = build_dnn(3, 4, [2,2])\n",
        "    t = {\"x\": graph.get_tensor_by_name('placeholders/Xin:0'), \n",
        "        \"y\": graph.get_tensor_by_name('placeholders/Yin:0'), \n",
        "        \"dropout_rate\": graph.get_tensor_by_name('placeholders/dropout_rate:0'), \n",
        "        \"cost\": graph.get_tensor_by_name('optimize/cost:0'), \n",
        "        \"accuracy\": graph.get_tensor_by_name('metrics/accuracy:0'), \n",
        "        \"out\": graph.get_tensor_by_name('output/y_pred:0')}\n",
        "  return (dnn, t)\n",
        "\n",
        "\n",
        "get_tensors_ex()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvbFTWk3WdHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ex(nsample=None):\n",
        "  ds = datasets.load_digits()\n",
        "  \n",
        "  batch_size = 128\n",
        "  learning_rate = 0.0001\n",
        "  hidden_layers = [256,256]\n",
        " \n",
        "  if nsample==None:\n",
        "    xsample = ds.data\n",
        "    ysample = ds.target\n",
        "  else:\n",
        "    sel_idx = random.sample(range(1, ds.data.shape[0]), nsample)\n",
        "    xsample = ds.data[sel_idx]\n",
        "    ysample = ds.target[sel_idx]\n",
        "    \n",
        "  # normalization\n",
        "  xsample = np.apply_along_axis((lambda x: (x - x.mean()) / x.std()),\n",
        "    axis=1, arr=xsample.astype('float32'))\n",
        "\n",
        "  # one-hot encoding the target\n",
        "  y_labels, y_dummies = one_hot(ds.target_names, ysample)\n",
        "  \n",
        "  # split dataset into train and test\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "    xsample, y_dummies, test_size=0.3)\n",
        "  \n",
        "  # normalize training data\n",
        "  # x_train = np.apply_along_axis((lambda x: (x - x.mean()) / x.std()),\n",
        "  #   axis=1, arr=x_train.astype('float32'))\n",
        "\n",
        "  train_size, num_features = x_train.shape\n",
        "  num_labels = ds.target_names.shape[0]\n",
        "  \n",
        "  # build classifier\n",
        "  (_train, _evaluate, _predict) = build_classifier_estimator(\n",
        "      num_features, num_labels, hidden_layers, 200)\n",
        "\n",
        "\n",
        "  def train(epochs=300):\n",
        "    _train(x_train, y_train, batch_size, epochs, learning_rate, 0.45, 0.1, 4)\n",
        "\n",
        "\n",
        "  def evaluate():\n",
        "    (cost_val, accuracy_val) = _evaluate(x_train, y_train, batch_size)\n",
        "    print(\"Train metric:\")\n",
        "    print({\"accuracy\": accuracy_val, \"loss\": cost_val})\n",
        "    \n",
        "    (cost_val, accuracy_val) = _evaluate(x_test, y_test, batch_size)\n",
        "    print(\"Test metric:\")\n",
        "    print({\"accuracy\": accuracy_val, \"loss\": cost_val})\n",
        "\n",
        "\n",
        "  def predict():\n",
        "    df = _predict(x_test, ds.target_names, batch_size)\n",
        "    ydata = ds.target_names[np.argmax(y_test, axis=1)]\n",
        "    df[\"ydata\"] = ydata\n",
        "    return df\n",
        "\n",
        "\n",
        "  return (train, evaluate, predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M7keRw4XYKm",
        "colab_type": "code",
        "outputId": "0bf3f5a9-9cda-4168-bf4a-f45a690e7bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "(ex_train, ex_eval, ex_predict) = ex()\n",
        "ex_train(1000)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 \t| Loss = 299.53745 \t| Accuracy = 0.109375\n",
            "Iteration 200 \t| Loss = 8.64798 \t| Accuracy = 0.984375\n",
            "Iteration 400 \t| Loss = 2.4611244 \t| Accuracy = 0.9921875\n",
            "Iteration 600 \t| Loss = 1.0726773 \t| Accuracy = 1.0\n",
            "Iteration 800 \t| Loss = 0.23384434 \t| Accuracy = 1.0\n",
            "Iteration 1000 \t| Loss = 0.15774536 \t| Accuracy = 1.0\n",
            "Iteration 1200 \t| Loss = 0.07114814 \t| Accuracy = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clG8Da9B7Cl2",
        "colab_type": "code",
        "outputId": "12cfda4f-b4de-4da9-cb68-e9b8a4e7d97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "ex_eval()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train metric:\n",
            "{'accuracy': 1.0, 'loss': 0.0009413301944732666}\n",
            "Test metric:\n",
            "{'accuracy': 0.9828125, 'loss': 0.12605146169662476}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK9oMygO1yqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = ex_predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPlJRTYig97x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "d77ae4a7-44a2-45bd-b0cc-0e9bd8c12d7f"
      },
      "source": [
        "print(\"Correct prediction:\")\n",
        "df.loc[df[\"ypred\"] == df[\"ydata\"]]\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002</td>\n",
              "      <td>0.948</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.026</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>529 rows Ã— 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0      1    2      3      4  ...    7      8      9  ypred  ydata\n",
              "0    0.000  0.000  1.0  0.000  0.000  ...  0.0  0.000  0.000      2      2\n",
              "1    0.000  0.000  0.0  0.000  0.000  ...  1.0  0.000  0.000      7      7\n",
              "2    0.002  0.948  0.0  0.002  0.001  ...  0.0  0.004  0.026      1      1\n",
              "4    0.000  0.000  0.0  0.000  0.000  ...  0.0  1.000  0.000      8      8\n",
              "5    0.000  0.000  0.0  0.000  0.000  ...  1.0  0.000  0.000      7      7\n",
              "..     ...    ...  ...    ...    ...  ...  ...    ...    ...    ...    ...\n",
              "535  0.000  0.000  0.0  0.000  0.000  ...  0.0  0.000  1.000      9      9\n",
              "536  0.000  0.000  0.0  1.000  0.000  ...  0.0  0.000  0.000      3      3\n",
              "537  0.000  0.000  0.0  1.000  0.000  ...  0.0  0.000  0.000      3      3\n",
              "538  0.000  0.999  0.0  0.000  0.000  ...  0.0  0.000  0.000      1      1\n",
              "539  0.000  0.000  0.0  1.000  0.000  ...  0.0  0.000  0.000      3      3\n",
              "\n",
              "[529 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO_2YG5RhEvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "e7a5cbec-676d-4e30-82e0-9aa9e32243f5"
      },
      "source": [
        "print(\"Wrong prediction:\")\n",
        "df.loc[df[\"ypred\"] != df[\"ydata\"]]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.243</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.020</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.630</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.105</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>0.465</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.905</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.929</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.729</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0      1      2      3      4  ...      7      8      9  ypred  ydata\n",
              "3    0.000  0.000  0.000  0.004  0.000  ...  0.000  0.752  0.243      8      9\n",
              "55   0.020  0.601  0.000  0.000  0.024  ...  0.004  0.039  0.040      1      8\n",
              "86   0.000  0.006  0.000  0.139  0.000  ...  0.000  0.000  0.005      5      3\n",
              "97   0.010  0.022  0.000  0.000  0.093  ...  0.000  0.104  0.105      5      1\n",
              "112  0.465  0.000  0.000  0.000  0.494  ...  0.000  0.001  0.000      4      0\n",
              "147  0.010  0.000  0.000  0.001  0.001  ...  0.025  0.001  0.905      9      7\n",
              "217  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000  1.000      9      5\n",
              "252  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000  0.000      6      5\n",
              "309  0.000  0.929  0.000  0.000  0.002  ...  0.000  0.004  0.000      1      8\n",
              "413  0.000  0.000  0.802  0.008  0.000  ...  0.000  0.190  0.000      2      3\n",
              "436  0.000  0.022  0.000  0.000  0.729  ...  0.002  0.231  0.000      4      8\n",
              "\n",
              "[11 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}