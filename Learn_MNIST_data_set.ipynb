{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learn MNIST data set.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FCZBMVPPU5na"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t2wain/colab/blob/master/Learn_MNIST_data_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oVfh3gDPQg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZBMVPPU5na",
        "colab_type": "text"
      },
      "source": [
        "# Inspecting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkjcgVWmQMiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "digits = datasets.load_digits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAgiAhWIRtwy",
        "colab_type": "code",
        "outputId": "66a9c668-0106-4250-c38f-f38ecac4b8f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(\"data:\", digits.data.shape, type(digits.data))\n",
        "print(\"target:\", digits.target.shape, type(digits.target))\n",
        "print(\"target_names:\", digits.target_names.shape, type(digits.target_names))\n",
        "print(\"images:\", digits.images.shape, type(digits.images))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: (1797, 64) <class 'numpy.ndarray'>\n",
            "target: (1797,) <class 'numpy.ndarray'>\n",
            "target_names: (10,) <class 'numpy.ndarray'>\n",
            "images: (1797, 8, 8) <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9BWnNASTpYh",
        "colab_type": "code",
        "outputId": "4be5fae4-d1ff-4a72-a88e-2ba3c9f4e3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
        "    plt.subplot(2, 5, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('Training: %i' % label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADfCAYAAADWQznrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW5IKY3BLUmEMbkkqjMEtSYX5f+JS8Lp4AVjOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97UWz8OV3vb",
        "colab_type": "text"
      },
      "source": [
        "# Example 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6ljKYDV65D",
        "colab_type": "text"
      },
      "source": [
        "## Utility methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zm1II56Y3Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(X, Y=None, batch_size=1, epochs=1):\n",
        "  X_size, _ = X.shape\n",
        "\n",
        "  for ep in range(epochs):\n",
        "    for step in range(X_size // batch_size):\n",
        "      offset = (step * batch_size) % X_size\n",
        "      batch_x = X[offset:(offset + batch_size), :]\n",
        "      batch_y = None if Y is None else Y[offset:(offset + batch_size)]\n",
        "      yield (batch_x, batch_y)\n",
        "    remainder = X_size % batch_size\n",
        "    if remainder > 0:\n",
        "      offset = X_size - remainder\n",
        "      batch_x = X[offset:, :]\n",
        "      batch_y = None if Y is None else Y[offset:]\n",
        "      yield (batch_x, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4m55J7ln6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dnn(num_features, num_labels, hiddens=[]):\n",
        "  \n",
        "  def loss(Y, y_prob):\n",
        "    xentropy = -tf.reduce_sum(Y * tf.log(y_prob), reduction_indices=1)\n",
        "    loss = tf.reduce_mean(xentropy, name='cost')\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def loss_logit(Y, logit):\n",
        "    xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)\n",
        "    loss = tf.reduce_mean(tf.reduce_sum(xentropy), name='cost')\n",
        "    #loss = tf.reduce_sum(xentropy, name='cost')\n",
        "    return loss\n",
        "\n",
        "\n",
        "  num_sample = None\n",
        "  with tf.name_scope('placeholders'):\n",
        "    Xin = tf.placeholder(tf.float32, shape=[num_sample, num_features], name='Xin')\n",
        "    Yin = tf.placeholder(tf.float32, shape=[num_sample, num_labels], name='Yin')\n",
        "    rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
        "  \n",
        "  X = Xin; W = None; layers = []; y_out = None\n",
        "  # create nodes for hidden layers\n",
        "  num_input = num_features\n",
        "  num_hidden_layers = len(hiddens)\n",
        "\n",
        "  for layer_num, num_node_output in enumerate(hiddens):\n",
        "    layer_name = \"hidden_layer_%d\" % (layer_num+1)\n",
        "    with tf.name_scope(layer_name):\n",
        "      with tf.variable_scope(layer_name):\n",
        "        W = tf.get_variable('W', \n",
        "          initializer=tf.truncated_normal([num_input, num_node_output], stddev=0.1))\n",
        "        b = tf.get_variable('b', \n",
        "          initializer=tf.Variable(tf.constant(0.1,shape=[num_node_output])))\n",
        "      fx = tf.add(tf.matmul(X, W), b) # linear regression\n",
        "      y_out = tf.nn.relu6(fx) # activation funtion to introduce non-linearity\n",
        "      # apply dropout to hidden layer to introduce regularization of W\n",
        "      layer_drop = tf.nn.dropout(y_out, rate=rate, name='y_out')\n",
        "      y_out = layer_drop\n",
        "      # keeping track of each layer\n",
        "      layers.append({\"W\": W, \"b\": b, \"out\": y_out, \"W_val\": None, \"b_val\": None})\n",
        "      X = y_out # y become Xin to next layer\n",
        "      num_input = num_node_output\n",
        "  \n",
        "  # create output layer\n",
        "  num_input = num_features if W is None else num_input\n",
        "  num_node_output = num_labels\n",
        "  with tf.name_scope('output'):\n",
        "    with tf.variable_scope('output'):\n",
        "      W = tf.get_variable('W', \n",
        "        initializer=tf.truncated_normal([num_input, num_node_output], stddev=0.1))\n",
        "      b = tf.get_variable('b', \n",
        "        initializer=tf.constant(0.1,shape=[num_node_output]))\n",
        "    y_logit = tf.add(tf.matmul(X, W), b) # logistic regression, ln-odd\n",
        "    # convert to probability, exp-logit, and normalize to interval between 0 and 1\n",
        "    y_prob = tf.nn.softmax(y_logit, name='y_prob')\n",
        "    layers.append({\"W\": W, \"b\": b, \"out\": y_prob, \"W_val\": None, \"b_val\": None})\n",
        "    y_pred = tf.argmax(y_prob, 1, name='y_pred') # index of max probablity\n",
        "    y_label = tf.argmax(Yin, 1, name='y_label') # index of one-hot label\n",
        "\n",
        "  with tf.name_scope('optimize'):\n",
        "    cost = loss_logit(Yin, y_logit)\n",
        "    #cost = loss(Yin, y_prob)\n",
        "    op = tf.train.AdamOptimizer()\n",
        "    # minimize cost/loss\n",
        "    train_op = op.minimize(cost)\n",
        "  \n",
        "  with tf.name_scope('metrics'):\n",
        "    correct_prediction = tf.equal(y_pred, y_label)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name='accuracy')\n",
        "  \n",
        "  return {\"x\": Xin, \"y\": Yin, \"dropout_rate\": rate, \"layers\": layers, \n",
        "          \"cost\": cost, \"accuracy\": accuracy, \"y_prob\": y_prob, \"y_pred\": y_pred,\n",
        "          \"op\": op, \"train_op\": train_op}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-Zi4LkjF_Gs",
        "colab": {}
      },
      "source": [
        "def build_classifier_estimator(num_features, num_labels, hiddens=[], msg_per_steps=0):\n",
        "  is_saved = False\n",
        "  \n",
        "  g1 = tf.Graph()\n",
        "  with g1.as_default() as graph:\n",
        "    dnn = build_dnn(num_features, num_labels, hiddens)\n",
        "\n",
        "  # placeholders, input to the training\n",
        "  X = dnn[\"x\"]\n",
        "  Y = dnn[\"y\"]\n",
        "  rate = dnn[\"dropout_rate\"]\n",
        "\n",
        "  # optimizer\n",
        "  op = dnn[\"op\"]\n",
        "  train_op = dnn[\"train_op\"]\n",
        "\n",
        "  # metrics\n",
        "  cost = dnn[\"cost\"]\n",
        "  accuracy = dnn[\"accuracy\"]\n",
        "  y_pred = dnn[\"y_pred\"]\n",
        "  y_prob = dnn[\"y_prob\"]\n",
        "  \n",
        "\n",
        "  def train(xsample, ysample,  batch_size=1, training_epochs=1, \n",
        "            learning_rate=0.001, dropout_rate=0, max_loss=0.01, limit_count=4):\n",
        "    op.learning_rate = learning_rate\n",
        "    loss_count = 0\n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      step_iterator = enumerate(data_gen(xsample, ysample,  batch_size, training_epochs))\n",
        "      for step, (batch_xs, batch_labels) in step_iterator:\n",
        "        feed_dict = {X: batch_xs, Y: batch_labels, rate: dropout_rate}\n",
        "        sess.run(train_op, feed_dict=feed_dict)\n",
        "\n",
        "        # print training progress\n",
        "        if msg_per_steps > 0 and step % msg_per_steps == 0:\n",
        "          feed_dict = {X: batch_xs, Y: batch_labels, rate: 0}\n",
        "          cost_val, accuracy_val = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
        "          print(\n",
        "              \"Iteration\", str(step), \n",
        "              \"\\t| Loss =\", str(cost_val), \n",
        "              \"\\t| Accuracy =\", str(accuracy_val))\n",
        "          \n",
        "        # early training stop\n",
        "        if math.isnan(cost_val) or loss_count > limit_count:\n",
        "          break;\n",
        "        elif cost_val < max_loss:\n",
        "          loss_count += 1\n",
        "          \n",
        "      _saveVars(sess)\n",
        "\n",
        "      \n",
        "  def _restoreVars(sess):\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    if is_saved:\n",
        "      for layer in dnn[\"layers\"]:\n",
        "        sess.run(layer[\"W\"].assign(layer[\"W_val\"]))\n",
        "        sess.run(layer[\"b\"].assign(layer[\"b_val\"]))\n",
        "\n",
        "    \n",
        "  def _saveVars(sess):\n",
        "    nonlocal is_saved\n",
        "    is_saved = True\n",
        "    for layer in dnn[\"layers\"]:\n",
        "      layer[\"W_val\"] = sess.run(layer[\"W\"])\n",
        "      layer[\"b_val\"] = sess.run(layer[\"b\"])\n",
        "\n",
        "      \n",
        "  def evaluate(xsample, ysample, batch_size=1):\n",
        "    if not is_saved:\n",
        "      return \"Error: Training has not been done.\"\n",
        "\n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      cost_total = accuracy_total = step = 0\n",
        "      for step, (batch_xs, batch_ys) in enumerate(data_gen(xsample, ysample,  batch_size)):\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, rate: 0}\n",
        "        cost_val, accuracy_val = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
        "        cost_total += cost_val\n",
        "        accuracy_total += accuracy_val\n",
        "      return (cost_val/(step+1), accuracy_total/(step+1))\n",
        "\n",
        "    \n",
        "  def predict(xdata, labels, batch_size=1):\n",
        "    if not is_saved:\n",
        "      return \"Error: Training has not been done.\"\n",
        "      \n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      pred = []; prob = []\n",
        "      for (batch_xs, _) in data_gen(xdata, batch_size=batch_size):\n",
        "        feed_dict={X: batch_xs, rate: 0}\n",
        "        yprob = sess.run(y_prob, feed_dict=feed_dict)\n",
        "        ypred = np.argmax(yprob, axis=1)\n",
        "        pred.extend(ypred)\n",
        "        prob.extend(yprob.round(3))\n",
        "        \n",
        "      df = pd.DataFrame(prob, columns=labels)\n",
        "      plabel = labels[pred]\n",
        "      df[\"ypred\"] = plabel\n",
        "      return df\n",
        "\n",
        "    \n",
        "  return (train, evaluate, predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAS37b0NfoIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(labels, ydata):\n",
        "  y_target = np.expand_dims(ydata, axis=1)\n",
        "  enc = OneHotEncoder(categories=[labels])\n",
        "  enc.fit(y_target)\n",
        "  return (labels, enc.transform(y_target).toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdTWTZuAWbAP",
        "colab_type": "text"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWHbAea6ns4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tensors_ex():\n",
        "  g1 = tf.Graph()\n",
        "  with g1.as_default() as graph:\n",
        "    dnn = build_dnn(3, 4, [2,2])\n",
        "    t = {\"x\": graph.get_tensor_by_name('placeholders/Xin:0'), \n",
        "        \"y\": graph.get_tensor_by_name('placeholders/Yin:0'), \n",
        "        \"dropout_rate\": graph.get_tensor_by_name('placeholders/dropout_rate:0'), \n",
        "        \"cost\": graph.get_tensor_by_name('optimize/cost:0'), \n",
        "        \"accuracy\": graph.get_tensor_by_name('metrics/accuracy:0'), \n",
        "        \"out\": graph.get_tensor_by_name('output/y_pred:0')}\n",
        "  return (dnn, t)\n",
        "\n",
        "\n",
        "get_tensors_ex()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaDLw1PCwJj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(nsample=None):\n",
        "  ds = datasets.load_digits()\n",
        "  \n",
        "  # build dataframe\n",
        "  df = pd.DataFrame(ds.data)\n",
        "  df[\"ydata\"] = ds.target\n",
        "  \n",
        "  # get subset of data\n",
        "  if not nsample==None:\n",
        "    df = df.sample(nsample)\n",
        "    \n",
        "  # normalization\n",
        "  df.iloc[:,:-1] = df.iloc[:,:-1].apply((lambda x: (x - x.mean()) / x.std()), axis=1)\n",
        "  \n",
        "  # one-hot encoding the target\n",
        "  y_labels, y_dummies = one_hot(ds.target_names, df[\"ydata\"].to_numpy())\n",
        "  dhot = pd.DataFrame(y_dummies, columns=y_labels)\n",
        "    \n",
        "  # split dataset into train and test\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df, dhot, test_size=0.3)\n",
        "\n",
        "  return (x_train, x_test, y_train, y_test, ds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvbFTWk3WdHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ex(nsample=None):\n",
        "  batch_size = 128\n",
        "  learning_rate = 0.0001\n",
        "  hidden_layers = [256,256]\n",
        "  \n",
        "  (x_train_ds, x_test_ds, y_train_ds, y_test_ds, ds) = get_data(nsample)\n",
        "  x_train = x_train_ds.iloc[:,:-1].to_numpy()\n",
        "  x_test = x_test_ds.iloc[:,:-1].to_numpy()\n",
        "  y_train = y_train_ds.to_numpy()\n",
        "  y_test = y_test_ds.to_numpy()\n",
        "\n",
        "  train_size, num_features = x_train.shape\n",
        "  num_labels = ds.target_names.shape[0]\n",
        "  \n",
        "  # build classifier\n",
        "  (_train, _evaluate, _predict) = build_classifier_estimator(\n",
        "      num_features, num_labels, hidden_layers, 200)\n",
        "\n",
        "\n",
        "  def train(epochs=300, max_loss=0.01):\n",
        "    _train(x_train, y_train, batch_size, epochs, learning_rate, 0.45, max_loss, 4)\n",
        "\n",
        "\n",
        "  def evaluate():\n",
        "    (cost_val, accuracy_val) = _evaluate(x_train, y_train, batch_size)\n",
        "    print(\"Train metric:\")\n",
        "    print({\"accuracy\": accuracy_val, \"loss\": cost_val})\n",
        "    \n",
        "    (cost_val, accuracy_val) = _evaluate(x_test, y_test, batch_size)\n",
        "    print(\"Test metric:\")\n",
        "    print({\"accuracy\": accuracy_val, \"loss\": cost_val})\n",
        "\n",
        "\n",
        "  def predict():\n",
        "    df = _predict(x_test, ds.target_names, batch_size)\n",
        "    df.index = x_test_ds.index\n",
        "    ydata = ds.target_names[np.argmax(y_test, axis=1)]\n",
        "    df[\"ydata\"] = ydata\n",
        "    df[\"max_prob\"] = df.iloc[:, 0:10].apply(lambda x: x.max(), axis=1)\n",
        "    return df\n",
        "  \n",
        "  def plot_image(df_pred):\n",
        "    idx = df_pred.index.to_numpy()\n",
        "    images_labels_pred = list(zip(ds.images[idx], ds.target[idx], df_pred[\"ypred\"].to_numpy()))\n",
        "    for index, (image, label, pred) in enumerate(images_labels_pred[:10]):\n",
        "        plt.subplot(2, 5, index + 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "        plt.title('P:%i, D:%i' % (pred, label))\n",
        "\n",
        "\n",
        "  return (train, evaluate, predict, plot_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M7keRw4XYKm",
        "colab_type": "code",
        "outputId": "628fe4fa-6360-48ba-f9d9-582f9f6d04a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "(ex_train, ex_eval, ex_predict, plot_image) = ex()\n",
        "ex_train(1000, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 \t| Loss = 273.74927 \t| Accuracy = 0.1484375\n",
            "Iteration 200 \t| Loss = 2.8340466 \t| Accuracy = 1.0\n",
            "Iteration 400 \t| Loss = 0.7170165 \t| Accuracy = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clG8Da9B7Cl2",
        "colab_type": "code",
        "outputId": "627fcc98-17b7-4a93-bbb4-706425ec6538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "ex_eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train metric:\n",
            "{'accuracy': 0.99921875, 'loss': 0.06750921607017517}\n",
            "Test metric:\n",
            "{'accuracy': 0.984375, 'loss': 0.062192171812057495}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK9oMygO1yqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = ex_predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPlJRTYig97x",
        "colab_type": "code",
        "outputId": "fe32c7a0-a152-44c7-9ef5-4ddccb025cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "print(\"Correct prediction:\")\n",
        "df_correct = df.loc[df[\"ypred\"] == df[\"ydata\"]].sort_values([\"max_prob\"], axis=0, ascending=True)\n",
        "df_correct"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>0.213</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1210</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.016</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.083</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1412</th>\n",
              "      <td>0.035</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.439</td>\n",
              "      <td>0.522</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.553</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1153</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>530 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3      4  ...      8      9  ypred  ydata  max_prob\n",
              "492   0.213  0.017  0.001  0.000  0.015  ...  0.348  0.000      6      6     0.398\n",
              "1210  0.001  0.002  0.202  0.042  0.005  ...  0.442  0.016      8      8     0.442\n",
              "1468  0.001  0.323  0.001  0.046  0.006  ...  0.482  0.083      8      8     0.482\n",
              "1412  0.035  0.000  0.001  0.003  0.000  ...  0.439  0.522      9      9     0.522\n",
              "555   0.000  0.001  0.008  0.404  0.000  ...  0.011  0.553      9      9     0.553\n",
              "...     ...    ...    ...    ...    ...  ...    ...    ...    ...    ...       ...\n",
              "1744  0.000  0.000  1.000  0.000  0.000  ...  0.000  0.000      2      2     1.000\n",
              "1153  1.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      0      0     1.000\n",
              "987   0.000  1.000  0.000  0.000  0.000  ...  0.000  0.000      1      1     1.000\n",
              "866   0.000  1.000  0.000  0.000  0.000  ...  0.000  0.000      1      1     1.000\n",
              "1108  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      7      7     1.000\n",
              "\n",
              "[530 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11-Vb0mGR6Ss",
        "colab_type": "code",
        "outputId": "5fe2cac8-1eb4-4eb6-808f-62a3744a0709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "ax = sns.distplot(df_correct[\"max_prob\"], kde=False, rug=True, bins=10)\n",
        "ax.set_title(\"Correct Prediction Probability\")\n",
        "ax.set_ylabel(\"Image Count\")\n",
        "ax.set_xlabel(\"Prediction Probability\")\n",
        "plt.show()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAewUlEQVR4nO3deZgdZZn38e+PBAhLIEICE7IQlCCi\nImKG5QXGAKNCVILIKkKCaEZFZcRRMr4oiiCgr4MgKhMFCQzrsAYEFAMBWQIm7BCWAAlkIYQAYQtI\n8H7/qOcUlZM+3dWdPku6f5/rOldXPfVU1f2cc7ru2s5TigjMzMwA1mh2AGZm1jqcFMzMLOekYGZm\nOScFMzPLOSmYmVnOScHMzHJOCmY1SApJW6bhsyT9oIvLeU3Se7s3uu4l6UeS/qeL846XdFs706+X\nNK6tuqvDe9PbOCn0cpK+IGlG+udcmP6Bd22BuNrd0KQ60yS9mWJ/QdIVkgbXI56I+GpE/KSjeimm\nL1fNu35EPNXdMUmaI2lZav8iSedKWr+717OqImLviJhcY1r+3qT4T2xsdFbNSaEXk3QM8Evgp8Cm\nwHDgN8DYLiyrb5myOvhGRKwPbAUMAE5rq5KkPg2IpRk+m9q/PTAKOK66gjL+X7dS/EXppSRtCJwA\nHBURV0TE6xHxdkRcExHfTXXWlvRLSQvS65eS1k7TRkuaJ+lYSc8Bf2irLNX9jKT7JL0s6Q5J2xbi\nGJb28BdLWiLpTEkfAM4Cdk57wS931J6IeBG4HPhQWu65kn4r6TpJrwO7p/b8P0nPpD3rsyStU4jl\nu+loaYGkL1W9XyvsxUoam9r0iqQnJe0l6SRgN+DMFPeZqW7xNNSGks5L7Z0r6bjKBrtydJRifEnS\n05L2LvN5RsR84PpC+6dJOknS7cAbwHslbSZpiqQXJc2W9JWqxfSTdImkVyXdI+kjhfZOTO18VdIj\nkj5XNa/SZ7dU0qOS9ixMWOnoqTAtJG0paQJwKPC99N5dkz6Py6vqnyHp9DLviXVRRPjVC1/AXsBy\noG87dU4ApgObAIOAO4CfpGmj0/ynAmsD69Qo+yjwPLAj0AcYB8xJ0/sA95Pt3a8H9AN2TcsfD9zW\nQRumAV9OwwOBm4Dz0/i5wFJgF7Kdn35pPVOAjYD+wDXAyYX3YxHZRnU94EIggC0LyzsxDe+Qlv2J\ntOwhwNbVMRXiLC7nPODqtP4RwOPAkYU2vw18Jb03XwMWAKrR/jnAv6bhYcDDhc9nGvAM8EGgL7Am\ncCvZkWA/YDtgMbBHqv+jtO79U93/AJ4G1kzTDwA2S+09CHgdGFyIeznw7TTvQen92aiNz2mFz7XW\ne5zGB6f1DEjjfcm+Sx9r9v9PT341PQC/mvTBZ3tlz3VQ50lgTGH8U8CcNDwa+DvQrzC9rbLfVjZU\nhbLHgI8DO6cN00qJqXrjUSO+aWR7wS8D84ELgEFp2rnAeYW6ShuY9xXKdgaeTsPnAKcUpm1Va4MF\n/DdwWjsxtZkUyDb0fwe2KUz7N2Baoc2zC9PWTfP+U411zQFeS+2fS7bBX6cQxwmFusOAd4D+hbKT\ngXPT8I+A6YVpawALgd1qrPs+YGwh7hWSF3A3cFj1e1L9udZ6jwvTrwe+koY/AzzS7P+dnv5qxDlf\na01LgIGS+kbE8hp1NiPb2FTMTWUViyPizap5qss2B8ZJ+mahbK20nHeAue2sv4xvRcTva0x7tjA8\niGwjO1NSpUxkG2pSPDML9YvtrjYMuK7zoTKQbE+6+j0dUhh/rjIQEW+kWNu7eLxvRPylxrRi+zcD\nXoyIV6vWPaqt+hHxD0nz0nxIOhw4huzophLTwMK88yNtuQvLLn5Xumoy2RHT74AvAud3wzKtHb6m\n0HvdCbwF7NtOnQVkG/WK4amsoq0udqvLngVOiogBhde6EXFRmja8xgXp7ui+t7iMF4BlwAcLcWwY\n2UVayPaKhxXqD29nuc8C7yuxzmovkJ2iqX5P57czz6ooxrIA2EhS/3bWnbc/XecYCiyQtDnZRvkb\nwMYRMQB4iCypVgxRIduy8nels/FWXAVsK+lDZEcKF3RymdZJTgq9VEQsBX4I/FrSvpLWlbSmpL0l\n/SxVuwg4TtIgSQNT/c7ey/474KuSdkx3wawn6dNp43Q32cb4lFTeT9Iuab5FwFBJa61yY8n2fFMs\np0naBEDSEEmfSlUuBcZL2kbSusDx7SzubOAISXtKWiMtZ+tC3G3edx8R76T1nCSpf9rYHkPn39NO\ni4hnya4JnZze522BI6vW/TFJ+6Uk/e9kOw3Tya6xBNmpPiQdQbqgXbAJ8K30HToA+ACdP5pa6b1L\nR52XkV3juTsinunkMq2TnBR6sYj4BdlG6Tiyf/hnyfYGr0pVTgRmAA8ADwL3pLLOrGMG2YXTM4GX\ngNlk55UrG8nPkp1vfwaYR3aRErKLxg8Dz0l6oSvta8Oxaf3TJb0C/AV4f4rlerLbc29KdW5qp013\nA0eQXbheCtzCu3v/pwP7p7uHzmhj9m+SXdt4CriNbGN3ziq3rJxDyE7/LACuBI6vOvV0Ndn7/xJw\nGLBfZHekPQL8guzochHwYeD2qmXfBYwkOxo6Cdg/IpZ0Mr6zgW2U3aV2VaF8clqnTx01gFY8DWhm\n1lokDQceJbvg/kqz4+npfKRgZi0rXds4BrjYCaExfPeRmbUkSeuRna6aS/Y7EmsAnz4yM7OcTx+Z\nmVlutT59NHDgwBgxYkSzwzAzW63MnDnzhYgY1Na01TopjBgxghkzZjQ7DDOz1Yqkmr/Y9+kjMzPL\nOSmYmVnOScHMzHJOCmZmlnNSMDOznJOCmZnlnBTMzCxX16QgaY6kB9MDzmekso0k3SjpifT3Palc\n6aHcsyU9IGn7esZmZmYra8SRwu4RsV1EVB77NxGYGhEjgalpHGBvsv7YRwITyJ7ta2ZmDdSMXzSP\nJXvAO2QPz5hG9vCTsWQPWg+yh6AMkDQ4IhY2IUYzsw5deFfzHgT3hR3be2Js19X7SCGAP0uaKWlC\nKtu0sKF/Dtg0DQ9hxQeNz2PFB5oDIGmCpBmSZixevLhecZuZ9Ur1PlLYNSLmp2fi3ijp0eLEiAhJ\nneq7OyImAZMARo0a5X6/zcy6UV2PFCJifvr7PNkzYXcAFkkaDJD+Pp+qzweGFWYfmsrMzKxB6pYU\nJK0nqX9lGPgk8BAwBRiXqo0je1g4qfzwdBfSTsBSX08wM2usep4+2hS4UlJlPRdGxA2S/gZcKulI\nssfsHZjqXweMAWYDbwBH1DE2MzNrQ92SQkQ8BXykjfIlwJ5tlAdwVL3iMTOzjvkXzWZmlnNSMDOz\nnJOCmZnlnBTMzCznpGBmZjknBTMzyzkpmJlZzknBzMxyTgpmZpZzUjAzs5yTgpmZ5ZwUzMws56Rg\nZmY5JwUzM8s5KZiZWc5JwczMck4KZmaWc1IwM7Ock4KZmeWcFMzMLOekYGZmOScFMzPLOSmYmVnO\nScHMzHJOCmZmlnNSMDOznJOCmZnlnBTMzCznpGBmZjknBTMzyzkpmJlZzknBzMxydU8KkvpIulfS\ntWl8C0l3SZot6RJJa6XytdP47DR9RL1jMzOzFTXiSOFoYFZh/FTgtIjYEngJODKVHwm8lMpPS/XM\nzKyB6poUJA0FPg38Po0L2AO4LFWZDOybhsemcdL0PVN9MzNrkHofKfwS+B7wjzS+MfByRCxP4/OA\nIWl4CPAsQJq+NNVfgaQJkmZImrF48eJ6xm5m1uvULSlI+gzwfETM7M7lRsSkiBgVEaMGDRrUnYs2\nM+v1+tZx2bsA+0gaA/QDNgBOBwZI6puOBoYC81P9+cAwYJ6kvsCGwJI6xmdmZlXqdqQQEf8ZEUMj\nYgRwMHBTRBwK3Azsn6qNA65Ow1PSOGn6TRER9YrPzMxW1ozfKRwLHCNpNtk1g7NT+dnAxqn8GGBi\nE2IzM+vV6nn6KBcR04BpafgpYIc26rwJHNCIeMzMrG3+RbOZmeWcFMzMLOekYGZmOScFMzPLOSmY\nmVnOScHMzHJOCmZmlnNSMDOznJOCmZnlnBTMzCznpGBmZjknBTMzyzkpmJlZzknBzMxyTgpmZpZz\nUjAzs5yTgpmZ5ZwUzMws56RgZmY5JwUzM8t1mBQkHV2mzMzMVn9ljhTGtVE2vpvjMDOzFtC31gRJ\nhwBfALaQNKUwqT/wYr0DMzOzxquZFIA7gIXAQOAXhfJXgQfqGZSZmTVHzaQQEXOBucDOjQvHzMya\nqcyF5v0kPSFpqaRXJL0q6ZVGBGdmZo3V3umjip8Bn42IWfUOxszMmqvM3UeLnBDMzHqHMkcKMyRd\nAlwFvFUpjIgr6haVmZk1RZmksAHwBvDJQlkATgpmZj1Mh0khIo5oRCBmZtZ8HSYFSX8gOzJYQUR8\nqS4RmZlZ05S50Hwt8Mf0mkp2Oum1jmaS1E/S3ZLul/SwpB+n8i0k3SVptqRLJK2VytdO47PT9BFd\nbZSZmXVNmdNHlxfHJV0E3FZi2W8Be0TEa5LWBG6TdD1wDHBaRFws6SzgSOC36e9LEbGlpIOBU4GD\nOtccMzNbFV3pOnsksElHlSJTOaJYM70C2AO4LJVPBvZNw2PTOGn6npLUhfjMzKyLylxTeJVsY670\n9zng2DILl9QHmAlsCfwaeBJ4OSKWpyrzgCFpeAjwLEBELJe0FNgYeKFqmROACQDDhw8vE4aZmZVU\n5vRR/64uPCLeAbaTNAC4Eti6q8sqLHMSMAlg1KhRK10ANzOzrivzOwUk7QP8SxqdFhHXdmYlEfGy\npJvJOtcbIKlvOloYCsxP1eYDw4B5kvoCGwJLOrMeMzNbNWU6xDsFOBp4JL2OlvTTEvMNSkcISFoH\n+AQwC7gZ2D9VGwdcnYan8O4DffYHbooIHwmYmTVQmSOFMcB2EfEPAEmTgXuB73cw32BgcrqusAZw\naURcK+kR4GJJJ6blnJ3qnw2cL2k22UN8Du50a8zMbJWUOn0EDODdp61tWGaGiHgA+Ggb5U8BO7RR\n/iZwQMl4zMysDsokhZOBe9M1AZFdW5hY16jMzKwpytx9dJGkacA/p6JjI+K5ukZlZmZNUTMpSPoU\n0D8iLouIhWQXgpG0v6SlEXFjo4I0M7PGaO/uox8Ct7RRPg04oS7RmJlZU7WXFNaOiMXVhRHxArBe\n/UIyM7NmaS8pbJB+RLaC1LndOvULyczMmqW9pHAF8DtJ+VGBpPWBs/BT18zMeqT2ksJxwCJgrqSZ\nkmYCTwOL0zQzM+that59lPommpgejrNlKp4dEcsaEpmZmTVcmd8pLAMebEAsZmbWZF15yI6ZmfVQ\nTgpmZpYr03W2JH1R0g/T+HBJK3VoZ2Zmq78yRwq/IXs4ziFp/FWyR2uamVkPU6aX1B0jYntJ9wJE\nxEuS1qpzXGZm1gRljhTeTg/KCcieqAb8o65RmZlZU5RJCmcAVwKbSDoJuA3o8HGcZma2+inzO4UL\n0q+Z9yR7yM6+ETGr7pGZmVnDdZgUJG0EPA9cVChbMyLermdgZmbWeGVOH91D1t/R48ATaXiOpHsk\nfayewZmZWWOVSQo3AmMiYmBEbAzsDVwLfJ3sdlUzM+shyiSFnSLiT5WRiPgzsHNETAfWrltkZmbW\ncGV+p7BQ0rHAxWn8IGBRuk3Vt6aamfUgZY4UvgAMBa5Kr+GprA9wYP1CMzOzRitzS+oLwDdrTJ7d\nveGYmVkzlbkldRDwPeCDQL9KeUTsUce4zMysCcqcProAeBTYAvgxMAf4Wx1jMjOzJimTFDaOiLOB\ntyPiloj4EuCjBDOzHqjM3UeVXy4vlPRpYAGwUf1CMjOzZimTFE6UtCHwHeBXwAbAt+salZmZNUWZ\nu4+uTYNLgd3rG46ZmTVTmbuPtiC7JXVEsX5E7FO/sMzMrBnKnD66CjgbuIZO/IJZ0jDgPGBTsgf0\nTIqI01Ovq5eQJZk5wIHpaW4CTgfGAG8A4yPinvJNMTOzVVUmKbwZEWd0YdnLge9ExD2S+gMzJd0I\njAemRsQpkiYCE4FjyTraG5leOwK/TX/NzKxByiSF0yUdD/wZeKtS2NFefEQsBBam4VclzQKGAGOB\n0anaZGAaWVIYC5wXEQFMlzRA0uC0HDMza4AySeHDwGFkv02onD4KOvFbBUkjgI8CdwGbFjb0z5Gd\nXoIsYTxbmG1eKlshKUiaAEwAGD58eNkQzMyshDJJ4QDgvRHx966sQNL6wOXAv0fEK9mlg0xEhKTo\nzPIiYhIwCWDUqFGdmtfMzNpX5hfNDwEDurJwSWuSJYQLIuKKVLxI0uA0fTDZoz4B5gPDCrMPTWVm\nZtYgZZLCAOBRSX+SNKXy6mimdDfR2cCsiPivwqQpwLg0PA64ulB+uDI7AUt9PcHMrLHKnD46vovL\n3oXsWsSDku5LZd8HTgEulXQkMJd3n8lwHdntqLPJbkk9oovrNTOzLirzi+ZburLgiLgNUI3Je7ZR\nP4CjurIuMzPrHjWTgqRXye4yWmkS2TZ8g7pFZWZmTVEzKURE/0YGYmZmzVfmQrOZmfUSTgpmZpZz\nUjAzs5yTgpmZ5ZwUzMws56RgZmY5JwUzM8s5KZiZWc5JwczMck4KZmaWc1IwM7Ock4KZmeWcFMzM\nLOekYGZmOScFMzPLOSmYmVnOScHMzHJOCmZmlnNSMDOznJOCmZnlnBTMzCznpGBmZjknBTMzyzkp\nmJlZzknBzMxyTgpmZpZzUjAzs5yTgpmZ5ZwUzMws56RgZma5uiUFSedIel7SQ4WyjSTdKOmJ9Pc9\nqVySzpA0W9IDkravV1xmZlZbPY8UzgX2qiqbCEyNiJHA1DQOsDcwMr0mAL+tY1xmZlZD3ZJCRNwK\nvFhVPBaYnIYnA/sWys+LzHRggKTB9YrNzMza1uhrCptGxMI0/BywaRoeAjxbqDcvlZmZWQM17UJz\nRAQQnZ1P0gRJMyTNWLx4cR0iMzPrvRqdFBZVTgulv8+n8vnAsEK9oalsJRExKSJGRcSoQYMG1TVY\nM7PeptFJYQowLg2PA64ulB+e7kLaCVhaOM1kZmYN0rdeC5Z0ETAaGChpHnA8cApwqaQjgbnAgan6\ndcAYYDbwBnBEveIyM7Pa6pYUIuKQGpP2bKNuAEfVKxYzMyvHv2g2M7Ock4KZmeWcFMzMLOekYGZm\nOScFMzPLOSmYmVnOScHMzHJOCmZmlnNSMDOznJOCmZnlnBTMzCznpGBmZjknBTMzyzkpmJlZzknB\nzMxyTgpmZpZzUjAzs5yTgpmZ5ZwUzMws56RgZmY5JwUzM8s5KZiZWc5JwczMck4KZmaWc1IwM7Nc\nr00Kp934+Gqz7FZbXnH+er6PHa270Zqx7vbW2Z3xlF1PV9dZma87Yj7txsfzV3txHfTfd9act6M4\nq8c7mg/gL7MWAXDqDbPy8eKrMq0yXHHqDbOYdOuT/OCqB/PhU2+YtUL5j695iB9c9SDfv/Ld14iJ\nf2w3nq7qtUnh9KlPrDbLbrXlFeev5/vY0bobrRnrbm+d3RlP2fV0dZ2V+boj5tOnPpG/2ovrrqdf\nrDlvR3FWj3c0H8BNjz4PwNJly/Px4qsyrTJcsXTZcuYseYN34t3hpcuWr1D+1vLgnWh39d2m1yYF\nMzNbmZOCmZnlnBTMzCznpGBmZjknBTMzy/VtdgBmZt3hwrueKVXWXnlb04rj7c3XU7TUkYKkvSQ9\nJmm2pInNjsfMrLdpmSMFSX2AXwOfAOYBf5M0JSIeaW5kZlZGR3vRlendubfdG/bcG61lkgKwAzA7\nIp4CkHQxMBaoW1Ko5xequ5fdastr5iF1MzcEzVh3Z051NGI93hD3bIpo0M/kOiBpf2CviPhyGj8M\n2DEivlFVbwIwIY2+H3hsFVY7EHhhFeZvJT2lLT2lHeC2tKKe0g5YtbZsHhGD2prQSkcKpUTEJGBS\ndyxL0oyIGNUdy2q2ntKWntIOcFtaUU9pB9SvLa10oXk+MKwwPjSVmZlZg7RSUvgbMFLSFpLWAg4G\npjQ5JjOzXqVlTh9FxHJJ3wD+BPQBzomIh+u82m45DdUiekpbeko7wG1pRT2lHVCntrTMhWYzM2u+\nVjp9ZGZmTeakYGZmuV6RFMp2nyHp85JCUsvestZRWySNl7RY0n3p9eVmxNmRMp+JpAMlPSLpYUkX\nNjrGskp8JqcVPo/HJb3cjDg7UqIdwyXdLOleSQ9IGtOMOMso0ZbNJU1N7ZgmaWgz4uyIpHMkPS/p\noRrTJemM1M4HJG2/yiuNiB79Irto/STwXmAt4H5gmzbq9QduBaYDo5odd1fbAowHzmx2rN3QjpHA\nvcB70vgmzY57Vb5fhfrfJLuJoumxd+EzmQR8LQ1vA8xpdtyr0Jb/Bcal4T2A85sdd422/AuwPfBQ\njeljgOsBATsBd63qOnvDkULefUZE/B2odJ9R7SfAqcCbjQyuk8q2pdWVacdXgF9HxEsAEfE8ramz\nn8khwEUNiaxzyrQjgA3S8IbAggbG1xll2rINcFMavrmN6S0hIm4FVn7Y9LvGAudFZjowQNLgVVln\nb0gKQ4BnC+PzUlkuHXINi4g/NjKwLuiwLcnn06HkZZKGtTG92cq0YytgK0m3S5ouaa+GRdc5ZT8T\nJG0ObMG7G6NWUqYdPwK+KGkecB3ZUU8rKtOW+4H90vDngP6SNm5AbN2t9PevrN6QFNolaQ3gv4Dv\nNDuWbnINMCIitgVuBCY3OZ6u6kt2Cmk02d717yQNaGpEq+5g4LKIeKfZgXTRIcC5ETGU7LTF+en/\nZ3X0H8DHJd0LfJys94TV9XPpVqvrB9oZHXWf0R/4EDBN0hyy83JTWvRic4ddgUTEkoh4K43+HvhY\ng2LrjDJdmswDpkTE2xHxNPA4WZJoNZ3pnuVgWvPUEZRrx5HApQARcSfQj6xTtlZT5v9kQUTsFxEf\nBf5vKmvJGwA60O3dA/WGpNBu9xkRsTQiBkbEiIgYQXaheZ+ImNGccNvVYVcgVecT9wFmNTC+ssp0\naXIV2VECkgaSnU56qpFBllSqexZJWwPvAe5scHxllWnHM8CeAJI+QJYUFjc0ynLK/J8MLBzl/Cdw\nToNj7C5TgMPTXUg7AUsjYuGqLLBlurmol6jRfYakE4AZEbHa9K9Usi3fkrQPsJzsAtX4pgVcQ8l2\n/An4pKRHyA7rvxsRS5oXdds68f06GLg40i0jraZkO75Ddhrv22QXnce3YntKtmU0cLKkILvr8Kim\nBdwOSReRxTowXcs5HlgTICLOIru2MwaYDbwBHLHK62zBz9TMzJqkN5w+MjOzkpwUzMws56RgZmY5\nJwUzM8s5KZiZWc5JwVqCpHdSL6IPSfpfSeuuwrJGS7o2De9TqxfWNH2ApK8XxjeTdFlX11217Gmp\np877U3cd7+/C/KV/RKmsh9wza0y7I/0dUelxU9IoSWek4dGS/k9n4rOeyUnBWsWyiNguIj4E/B34\nanFi+nFOp7+vETElIk5pp8oA4OuF+gsiYv/Orqcdh0bER8i6G/l59URJfbpxXTVFxEob/IiYERHf\nSqOjAScFc1KwlvRXYMu0V/uYpPOAh4Bhkj4p6U5J96QjivUh7z//UUn38G5HZyvsPUvaVNKVac/9\n/rRnfArwvnSU8vOqPel+kv4g6UFlzxDYvbDMKyTdIOkJST8r0aZbgS3T/HMknZpiPUDSdso6/Xsg\nxfeewnyHFY6gdkjz75Deg3sl3VF1BDIsHWE8Ien4wvvwWnVAlSMqSSPIkvC307p2k/S0pDVTvQ2K\n49azOSlYS5HUF9gbeDAVjQR+ExEfBF4HjgP+NSK2B2YAx0jqB/wO+CxZX0//VGPxZwC3pD337YGH\ngYnAk+ko5btV9Y8CIiI+TNYZ3OS0LoDtgIOADwMHqePeaD9baBPAkojYPiIuBs4Djk2dGD5I9qvV\ninUjYjuyo5lKVwyPArulfnt+CPy0UH8H4PPAtmQJp8PTTxExBzgLOC29D38FpgGfTlUOBq6IiLc7\nWpat/np8Nxe22lhH0n1p+K/A2cBmwNzUTzxknRVuA9wuCbIHqNwJbA08HRFPAEj6H2BCG+vYAzgc\nIPVUurRqr7zarsCvUv1HJc0l64MJYGpELE3rewTYnBW7MK64QNIyYA4rdjV9SZp3Q2BARNySyieT\nPQCm4qK0/lvTHvsAsk4cJ0saSdbdRHEP/sZKdyCSrkht6Eo/Xr8HvkfWB9URZM+3sF7AScFaxbK0\nR5xLG/7Xi0VkG71DquqtMF+DvFUYfofa/0uH1uhc8fU2ytpS3Q9NkD0Q6uaI+Fw69TOtg/qdFhG3\np1Npo4E+EdHm4yCt5/HpI1udTAd2kVQ5N7+epK3ITqeMkPS+VO+QGvNPBb6W5u2T9tJfJdvzbstf\ngUNT/a2A4cBj3dGQinS08ZKk3VLRYcAthSoHpfXvStYD5lKyp55VukceX7XIT0jaSNI6wL7A7SVD\naet9OA+4EPhDyWVYD+CkYKuNiFhMthG8SNIDpFNHEfEm2emiP6aLt7Ue3Xk0sLukB4GZZM/tXUJ2\nOuohSdV3B/0GWCPVv4SsV9C36H7jgJ+nNm0HnFCY9qayB8GcRfY8A4CfkfXweS8rH6HcDVwOPABc\n3oku4K8BPle50JzKLiDr7rtVnwFhdeBeUs2sTZL2B8ZGxGHNjsUax9cUzGwlkn5FdhfYmGbHYo3l\nIwUzM8v5moKZmeWcFMzMLOekYGZmOScFMzPLOSmYmVnu/wNhPhATqZnQTwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO_2YG5RhEvr",
        "colab_type": "code",
        "outputId": "d843555c-6cf5-4068-a489-167f66d29362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "print(\"Wrong prediction:\")\n",
        "df_err = df.loc[df[\"ypred\"] != df[\"ydata\"]].sort_values([\"max_prob\"], axis=0, ascending=False)\n",
        "df_err"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.993</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.102</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>0.761</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.230</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>0.741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1662</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.689</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.215</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1658</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.005</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.565</td>\n",
              "      <td>0.359</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>0.565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1611</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.430</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>0.430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1690</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.005</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3      4  ...      8      9  ypred  ydata  max_prob\n",
              "1553  0.000  0.993  0.000  0.000  0.003  ...  0.003  0.000      1      8     0.993\n",
              "129   0.000  0.973  0.004  0.000  0.000  ...  0.023  0.000      1      8     0.973\n",
              "37    0.000  0.007  0.002  0.022  0.000  ...  0.002  0.102      5      9     0.864\n",
              "792   0.761  0.000  0.000  0.000  0.069  ...  0.001  0.000      0      6     0.761\n",
              "547   0.000  0.001  0.000  0.026  0.000  ...  0.741  0.230      8      9     0.741\n",
              "1662  0.000  0.062  0.001  0.001  0.005  ...  0.005  0.215      5      9     0.689\n",
              "1658  0.000  0.006  0.000  0.667  0.000  ...  0.319  0.005      3      9     0.667\n",
              "1660  0.000  0.001  0.000  0.000  0.072  ...  0.565  0.359      8      4     0.565\n",
              "1611  0.000  0.019  0.001  0.000  0.296  ...  0.234  0.430      9      4     0.430\n",
              "1690  0.001  0.014  0.000  0.403  0.000  ...  0.417  0.005      8      3     0.417\n",
              "\n",
              "[10 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSGPr4g279Hx",
        "colab_type": "code",
        "outputId": "eb182b57-c8c6-4182-8914-ba8505306d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "plot_image(df_err)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADOCAYAAACdDdHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6ZqZZeSka2aW0f8BaCZSeU6CmPEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F17Gvez03R1",
        "colab_type": "code",
        "outputId": "cc6da8f2-fb62-478c-a772-c5f93ab40285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "prob_threshold = 0.995\n",
        "print(\"Correct prediction with high probability\")\n",
        "df_correct.loc[df[\"max_prob\"] >= prob_threshold]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction with high probability\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1546</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1794</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1153</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>446 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0      1      2      3      4  ...      8      9  ypred  ydata  max_prob\n",
              "1546  0.0  0.995  0.000  0.000  0.005  ...  0.000  0.000      1      1     0.995\n",
              "745   0.0  0.995  0.000  0.000  0.002  ...  0.003  0.000      1      1     0.995\n",
              "1241  0.0  0.000  0.995  0.005  0.000  ...  0.000  0.000      2      2     0.995\n",
              "177   0.0  0.995  0.000  0.000  0.000  ...  0.000  0.005      1      1     0.995\n",
              "1794  0.0  0.001  0.000  0.000  0.000  ...  0.995  0.000      8      8     0.995\n",
              "...   ...    ...    ...    ...    ...  ...    ...    ...    ...    ...       ...\n",
              "1744  0.0  0.000  1.000  0.000  0.000  ...  0.000  0.000      2      2     1.000\n",
              "1153  1.0  0.000  0.000  0.000  0.000  ...  0.000  0.000      0      0     1.000\n",
              "987   0.0  1.000  0.000  0.000  0.000  ...  0.000  0.000      1      1     1.000\n",
              "866   0.0  1.000  0.000  0.000  0.000  ...  0.000  0.000      1      1     1.000\n",
              "1108  0.0  0.000  0.000  0.000  0.000  ...  0.000  0.000      7      7     1.000\n",
              "\n",
              "[446 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecrC9vsCEtgU",
        "colab_type": "code",
        "outputId": "3ac3d13f-36b8-46f7-c7e8-890281adb768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "print(\"Wrong prediction despite high probability\")\n",
        "df_err.loc[df[\"max_prob\"] >= prob_threshold]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction despite high probability\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ypred, ydata, max_prob]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    }
  ]
}