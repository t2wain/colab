{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learn MNIST data set.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FCZBMVPPU5na"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t2wain/colab/blob/master/Learn_MNIST_data_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oVfh3gDPQg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.datasets import mnist\n",
        "from keras import optimizers\n",
        "from keras import metrics\n",
        "from keras import losses\n",
        "from keras import callbacks\n",
        "import keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZBMVPPU5na",
        "colab_type": "text"
      },
      "source": [
        "# **Inspecting data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkjcgVWmQMiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "digits = datasets.load_digits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAgiAhWIRtwy",
        "colab_type": "code",
        "outputId": "66a9c668-0106-4250-c38f-f38ecac4b8f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(\"data:\", digits.data.shape, type(digits.data))\n",
        "print(\"target:\", digits.target.shape, type(digits.target))\n",
        "print(\"target_names:\", digits.target_names.shape, type(digits.target_names))\n",
        "print(\"images:\", digits.images.shape, type(digits.images))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: (1797, 64) <class 'numpy.ndarray'>\n",
            "target: (1797,) <class 'numpy.ndarray'>\n",
            "target_names: (10,) <class 'numpy.ndarray'>\n",
            "images: (1797, 8, 8) <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9BWnNASTpYh",
        "colab_type": "code",
        "outputId": "4be5fae4-d1ff-4a72-a88e-2ba3c9f4e3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
        "    plt.subplot(2, 5, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('Training: %i' % label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADfCAYAAADWQznrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW5IKY3BLUmEMbkkqjMEtSYX5f+JS8Lp4AVjOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WwgOxaUAnrhm"
      },
      "source": [
        "# **Example 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6ljKYDV65D",
        "colab_type": "text"
      },
      "source": [
        "### Utility methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zm1II56Y3Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(X, Y=None, batch_size=1, epochs=1):\n",
        "  X_size, _ = X.shape\n",
        "\n",
        "  for ep in range(epochs):\n",
        "    for step in range(X_size // batch_size):\n",
        "      offset = (step * batch_size) % X_size\n",
        "      batch_x = X[offset:(offset + batch_size), :]\n",
        "      batch_y = None if Y is None else Y[offset:(offset + batch_size)]\n",
        "      yield (batch_x, batch_y)\n",
        "    remainder = X_size % batch_size\n",
        "    if remainder > 0:\n",
        "      offset = X_size - remainder\n",
        "      batch_x = X[offset:, :]\n",
        "      batch_y = None if Y is None else Y[offset:]\n",
        "      yield (batch_x, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4m55J7ln6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dnn(num_features, num_labels, hiddens=[]):\n",
        "  \n",
        "  def loss(Y, y_prob):\n",
        "    xentropy = -tf.reduce_sum(Y * tf.log(y_prob), reduction_indices=1)\n",
        "    loss = tf.reduce_mean(xentropy, name='cost')\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def loss_logit(Y, logit):\n",
        "    xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)\n",
        "    loss = tf.reduce_mean(tf.reduce_sum(xentropy), name='cost')\n",
        "    #loss = tf.reduce_sum(xentropy, name='cost')\n",
        "    return loss\n",
        "\n",
        "\n",
        "  num_sample = None\n",
        "  with tf.name_scope('placeholders'):\n",
        "    Xin = tf.placeholder(tf.float32, shape=[num_sample, num_features], name='Xin')\n",
        "    Yin = tf.placeholder(tf.float32, shape=[num_sample, num_labels], name='Yin')\n",
        "    rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
        "  \n",
        "  X = Xin; W = None; layers = []; y_out = None\n",
        "  # create nodes for hidden layers\n",
        "  num_input = num_features\n",
        "  num_hidden_layers = len(hiddens)\n",
        "\n",
        "  for layer_num, num_node_output in enumerate(hiddens):\n",
        "    layer_name = \"hidden_layer_%d\" % (layer_num+1)\n",
        "    with tf.name_scope(layer_name):\n",
        "      with tf.variable_scope(layer_name):\n",
        "        W = tf.get_variable('W', \n",
        "          initializer=tf.truncated_normal([num_input, num_node_output], stddev=0.1))\n",
        "        b = tf.get_variable('b', \n",
        "          initializer=tf.Variable(tf.constant(0.1,shape=[num_node_output])))\n",
        "      fx = tf.add(tf.matmul(X, W), b) # linear regression\n",
        "      y_out = tf.nn.relu6(fx) # activation funtion to introduce non-linearity\n",
        "      # apply dropout to hidden layer to introduce regularization of W\n",
        "      layer_drop = tf.nn.dropout(y_out, rate=rate, name='y_out')\n",
        "      y_out = layer_drop\n",
        "      # keeping track of each layer\n",
        "      layers.append({\"W\": W, \"b\": b, \"out\": y_out, \"W_val\": None, \"b_val\": None})\n",
        "      X = y_out # y become Xin to next layer\n",
        "      num_input = num_node_output\n",
        "  \n",
        "  # create output layer\n",
        "  num_input = num_features if W is None else num_input\n",
        "  num_node_output = num_labels\n",
        "  with tf.name_scope('output'):\n",
        "    with tf.variable_scope('output'):\n",
        "      W = tf.get_variable('W', \n",
        "        initializer=tf.truncated_normal([num_input, num_node_output], stddev=0.1))\n",
        "      b = tf.get_variable('b', \n",
        "        initializer=tf.constant(0.1,shape=[num_node_output]))\n",
        "    y_logit = tf.add(tf.matmul(X, W), b) # logistic regression, ln-odd\n",
        "    # convert to probability, exp-logit, and normalize to interval between 0 and 1\n",
        "    y_prob = tf.nn.softmax(y_logit, name='y_prob')\n",
        "    layers.append({\"W\": W, \"b\": b, \"out\": y_prob, \"W_val\": None, \"b_val\": None})\n",
        "    y_pred = tf.argmax(y_prob, 1, name='y_pred') # index of max probablity\n",
        "    y_label = tf.argmax(Yin, 1, name='y_label') # index of one-hot label\n",
        "\n",
        "  with tf.name_scope('optimize'):\n",
        "    cost = loss_logit(Yin, y_logit)\n",
        "    #cost = loss(Yin, y_prob)\n",
        "    op = tf.train.AdamOptimizer()\n",
        "    # minimize cost/loss\n",
        "    train_op = op.minimize(cost)\n",
        "  \n",
        "  with tf.name_scope('metrics'):\n",
        "    correct_prediction = tf.equal(y_pred, y_label)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name='accuracy')\n",
        "  \n",
        "  return {\"x\": Xin, \"y\": Yin, \"dropout_rate\": rate, \"layers\": layers, \n",
        "          \"cost\": cost, \"accuracy\": accuracy, \"y_prob\": y_prob, \"y_pred\": y_pred,\n",
        "          \"op\": op, \"train_op\": train_op}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-Zi4LkjF_Gs",
        "colab": {}
      },
      "source": [
        "def build_classifier_estimator(num_features, num_labels, hiddens=[], msg_per_steps=0):\n",
        "  is_saved = False\n",
        "  \n",
        "  g1 = tf.Graph()\n",
        "  with g1.as_default() as graph:\n",
        "    dnn = build_dnn(num_features, num_labels, hiddens)\n",
        "\n",
        "  # placeholders, input to the training\n",
        "  X = dnn[\"x\"]\n",
        "  Y = dnn[\"y\"]\n",
        "  rate = dnn[\"dropout_rate\"]\n",
        "\n",
        "  # optimizer\n",
        "  op = dnn[\"op\"]\n",
        "  train_op = dnn[\"train_op\"]\n",
        "\n",
        "  # metrics\n",
        "  cost = dnn[\"cost\"]\n",
        "  accuracy = dnn[\"accuracy\"]\n",
        "  y_pred = dnn[\"y_pred\"]\n",
        "  y_prob = dnn[\"y_prob\"]\n",
        "  \n",
        "\n",
        "  def train(xsample, ysample,  batch_size=1, training_epochs=1, \n",
        "            learning_rate=0.001, dropout_rate=0, max_loss=0.01, limit_count=4):\n",
        "    op.learning_rate = learning_rate\n",
        "    loss_count = 0\n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      step_iterator = enumerate(data_gen(xsample, ysample,  batch_size, training_epochs))\n",
        "      for step, (batch_xs, batch_labels) in step_iterator:\n",
        "        feed_dict = {X: batch_xs, Y: batch_labels, rate: dropout_rate}\n",
        "        sess.run(train_op, feed_dict=feed_dict)\n",
        "\n",
        "        # print training progress\n",
        "        if msg_per_steps > 0 and step % msg_per_steps == 0:\n",
        "          feed_dict = {X: batch_xs, Y: batch_labels, rate: 0}\n",
        "          cost_val, accuracy_val = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
        "          print(\n",
        "              \"Iteration\", str(step), \n",
        "              \"\\t| Loss =\", str(cost_val), \n",
        "              \"\\t| Accuracy =\", str(accuracy_val))\n",
        "          \n",
        "        # early training stop\n",
        "        if math.isnan(cost_val) or loss_count > limit_count:\n",
        "          break;\n",
        "        elif cost_val < max_loss:\n",
        "          loss_count += 1\n",
        "          \n",
        "      _saveVars(sess)\n",
        "\n",
        "      \n",
        "  def _restoreVars(sess):\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    if is_saved:\n",
        "      for layer in dnn[\"layers\"]:\n",
        "        sess.run(layer[\"W\"].assign(layer[\"W_val\"]))\n",
        "        sess.run(layer[\"b\"].assign(layer[\"b_val\"]))\n",
        "\n",
        "    \n",
        "  def _saveVars(sess):\n",
        "    nonlocal is_saved\n",
        "    is_saved = True\n",
        "    for layer in dnn[\"layers\"]:\n",
        "      layer[\"W_val\"] = sess.run(layer[\"W\"])\n",
        "      layer[\"b_val\"] = sess.run(layer[\"b\"])\n",
        "\n",
        "      \n",
        "  def evaluate(xsample, ysample, batch_size=1):\n",
        "    if not is_saved:\n",
        "      return \"Error: Training has not been done.\"\n",
        "\n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      cost_total = accuracy_total = step = 0\n",
        "      for step, (batch_xs, batch_ys) in enumerate(data_gen(xsample, ysample,  batch_size)):\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, rate: 0}\n",
        "        cost_val, accuracy_val = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
        "        cost_total += cost_val\n",
        "        accuracy_total += accuracy_val\n",
        "      return (cost_val/(step+1), accuracy_total/(step+1))\n",
        "\n",
        "    \n",
        "  def predict(xdata, labels, batch_size=1):\n",
        "    if not is_saved:\n",
        "      return \"Error: Training has not been done.\"\n",
        "      \n",
        "    with tf.Session(graph=g1) as sess:\n",
        "      _restoreVars(sess)\n",
        "      pred = []; prob = []\n",
        "      for (batch_xs, _) in data_gen(xdata, batch_size=batch_size):\n",
        "        feed_dict={X: batch_xs, rate: 0}\n",
        "        yprob = sess.run(y_prob, feed_dict=feed_dict)\n",
        "        ypred = np.argmax(yprob, axis=1)\n",
        "        pred.extend(ypred)\n",
        "        prob.extend(yprob.round(3))\n",
        "        \n",
        "      df = pd.DataFrame(prob, columns=labels)\n",
        "      plabel = labels[pred]\n",
        "      df[\"ypred\"] = plabel\n",
        "      return df\n",
        "\n",
        "    \n",
        "  return (train, evaluate, predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAS37b0NfoIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(labels, ydata):\n",
        "  y_target = np.expand_dims(ydata, axis=1)\n",
        "  enc = OneHotEncoder(categories=[labels])\n",
        "  enc.fit(y_target)\n",
        "  return (labels, enc.transform(y_target).toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IllZaayjmdt_",
        "colab_type": "text"
      },
      "source": [
        "### Configure Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWHbAea6ns4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tensors_ex():\n",
        "  g1 = tf.Graph()\n",
        "  with g1.as_default() as graph:\n",
        "    dnn = build_dnn(3, 4, [2,2])\n",
        "    t = {\"x\": graph.get_tensor_by_name('placeholders/Xin:0'), \n",
        "        \"y\": graph.get_tensor_by_name('placeholders/Yin:0'), \n",
        "        \"dropout_rate\": graph.get_tensor_by_name('placeholders/dropout_rate:0'), \n",
        "        \"cost\": graph.get_tensor_by_name('optimize/cost:0'), \n",
        "        \"accuracy\": graph.get_tensor_by_name('metrics/accuracy:0'), \n",
        "        \"out\": graph.get_tensor_by_name('output/y_pred:0')}\n",
        "  return (dnn, t)\n",
        "\n",
        "\n",
        "get_tensors_ex()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaDLw1PCwJj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(nsample=None):\n",
        "  ds = datasets.load_digits()\n",
        "  \n",
        "  # build dataframe\n",
        "  df = pd.DataFrame(ds.data)\n",
        "  df[\"ydata\"] = ds.target\n",
        "  \n",
        "  # get subset of data\n",
        "  if not nsample==None:\n",
        "    df = df.sample(nsample)\n",
        "    \n",
        "  # normalization\n",
        "  df.iloc[:,:-1] = df.iloc[:,:-1].apply((lambda x: (x - x.mean()) / x.std()), axis=1)\n",
        "  \n",
        "  # one-hot encoding the target\n",
        "  y_labels, y_dummies = one_hot(ds.target_names, df[\"ydata\"].to_numpy())\n",
        "  dhot = pd.DataFrame(y_dummies, columns=y_labels)\n",
        "    \n",
        "  # split dataset into train and test\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df, dhot, test_size=0.3)\n",
        "\n",
        "  return (x_train, x_test, y_train, y_test, ds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvbFTWk3WdHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ex(nsample=None):\n",
        "  batch_size = 128\n",
        "  learning_rate = 0.0001\n",
        "  hidden_layers = [256,256]\n",
        "  \n",
        "  (x_train_ds, x_test_ds, y_train_ds, y_test_ds, ds) = get_data(nsample)\n",
        "  x_train = x_train_ds.iloc[:,:-1].to_numpy()\n",
        "  x_test = x_test_ds.iloc[:,:-1].to_numpy()\n",
        "  y_train = y_train_ds.to_numpy()\n",
        "  y_test = y_test_ds.to_numpy()\n",
        "\n",
        "  train_size, num_features = x_train.shape\n",
        "  num_labels = ds.target_names.shape[0]\n",
        "  \n",
        "  # build classifier\n",
        "  (_train, _evaluate, _predict) = build_classifier_estimator(\n",
        "      num_features, num_labels, hidden_layers, 200)\n",
        "\n",
        "\n",
        "  def train(epochs=300, max_loss=0.01):\n",
        "    _train(x_train, y_train, batch_size, epochs, learning_rate, 0.45, max_loss, 4)\n",
        "\n",
        "\n",
        "  def evaluate():\n",
        "    (cost_val, accuracy_val) = _evaluate(x_train, y_train, batch_size)\n",
        "    print(\"Train metric:\")\n",
        "    print({\"accuracy\": accuracy_val, \"loss\": cost_val})\n",
        "    \n",
        "    (cost_val, accuracy_val) = _evaluate(x_test, y_test, batch_size)\n",
        "    print(\"Test metric:\")\n",
        "    print({\"accuracy\": accuracy_val, \"loss\": cost_val})\n",
        "\n",
        "\n",
        "  def predict():\n",
        "    df = _predict(x_test, ds.target_names, batch_size)\n",
        "    df.index = x_test_ds.index\n",
        "    ydata = ds.target_names[np.argmax(y_test, axis=1)]\n",
        "    df[\"ydata\"] = ydata\n",
        "    df[\"max_prob\"] = df.iloc[:, 0:10].apply(lambda x: x.max(), axis=1)\n",
        "    return df\n",
        "  \n",
        "  def plot_image(df_pred, title):\n",
        "    idx = df_pred.index.to_numpy()\n",
        "    images_labels_pred = list(zip(ds.images[idx], ds.target[idx], df_pred[\"ypred\"].to_numpy()))\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    for index, (image, label, pred) in enumerate(images_labels_pred[:10]):\n",
        "        plt.subplot(2, 5, index + 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "        plt.title('P:%i, D:%i' % (pred, label))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  return (train, evaluate, predict, plot_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRUMD6u6mkdz",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M7keRw4XYKm",
        "colab_type": "code",
        "outputId": "f06281c1-b9d4-4193-d94d-aa8c797f0923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "(ex_train, ex_eval, ex_predict, plot_image) = ex()\n",
        "ex_train(1000, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 \t| Loss = 273.51886 \t| Accuracy = 0.21875\n",
            "Iteration 200 \t| Loss = 1.8662926 \t| Accuracy = 1.0\n",
            "Iteration 400 \t| Loss = 0.60827065 \t| Accuracy = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clG8Da9B7Cl2",
        "colab_type": "code",
        "outputId": "f27673f9-f7ff-437a-f1e7-198791891ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "ex_eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train metric:\n",
            "{'accuracy': 1.0, 'loss': 0.0319621205329895}\n",
            "Test metric:\n",
            "{'accuracy': 0.984375, 'loss': 0.04577687084674835}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKCH-ZZcm2_r",
        "colab_type": "text"
      },
      "source": [
        "## Analyze Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK9oMygO1yqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = ex_predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPlJRTYig97x",
        "colab_type": "code",
        "outputId": "6e1a3d77-6019-423b-bbe8-2cf1d5b7b8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "print(\"Correct prediction:\")\n",
        "df_correct = df.loc[df[\"ypred\"] == df[\"ydata\"]].sort_values([\"max_prob\"], axis=0, ascending=True)\n",
        "df_correct"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>0.137</td>\n",
              "      <td>0.337</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.023</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.012</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.638</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.083</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.733</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.744</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.026</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.771</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.001</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1759</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1332</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>530 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3      4  ...      8      9  ypred  ydata  max_prob\n",
              "1571  0.137  0.337  0.004  0.000  0.009  ...  0.459  0.023      8      8     0.459\n",
              "46    0.012  0.054  0.001  0.000  0.145  ...  0.055  0.083      5      5     0.638\n",
              "95    0.000  0.212  0.001  0.000  0.000  ...  0.053  0.000      6      6     0.733\n",
              "1602  0.000  0.013  0.004  0.744  0.000  ...  0.203  0.026      3      3     0.744\n",
              "1747  0.004  0.771  0.000  0.000  0.135  ...  0.079  0.001      1      1     0.771\n",
              "...     ...    ...    ...    ...    ...  ...    ...    ...    ...    ...       ...\n",
              "603   0.000  0.000  0.000  0.000  1.000  ...  0.000  0.000      4      4     1.000\n",
              "1759  0.000  0.000  0.000  0.000  0.000  ...  0.000  1.000      9      9     1.000\n",
              "112   0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      7      7     1.000\n",
              "1600  0.000  0.000  1.000  0.000  0.000  ...  0.000  0.000      2      2     1.000\n",
              "1332  0.000  0.000  0.000  1.000  0.000  ...  0.000  0.000      3      3     1.000\n",
              "\n",
              "[530 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11-Vb0mGR6Ss",
        "colab_type": "code",
        "outputId": "c202a40b-55ed-4119-db4e-0abd7eb7bac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "ax = sns.distplot(df_correct[\"max_prob\"], kde=False, rug=True, bins=10)\n",
        "ax.set_title(\"Correct Prediction Probability\")\n",
        "ax.set_ylabel(\"Image Count\")\n",
        "ax.set_xlabel(\"Prediction Probability\")\n",
        "ax.set_xlim(0,1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF3QE/VzQLoUHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO_2YG5RhEvr",
        "colab_type": "code",
        "outputId": "75bce9ef-df14-49d8-ab0a-bcd649595e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "print(\"Wrong prediction:\")\n",
        "df_err = df.loc[df[\"ypred\"] != df[\"ydata\"]].sort_values([\"max_prob\"], axis=0, ascending=False)\n",
        "df_err"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.993</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>0.993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.932</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0.932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1149</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.008</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0.823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1690</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0.017</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1658</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0.009</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>0.554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.395</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.007</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1727</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>0.134</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.395</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.000</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0.442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>922</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>0.431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>0.224</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0.373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3      4  ...      8      9  ypred  ydata  max_prob\n",
              "5     0.000  0.001  0.000  0.005  0.000  ...  0.000  0.993      9      5     0.993\n",
              "1361  0.000  0.000  0.000  0.000  0.000  ...  0.022  0.000      6      5     0.932\n",
              "1149  0.001  0.026  0.013  0.000  0.063  ...  0.036  0.008      7      8     0.823\n",
              "1690  0.000  0.023  0.003  0.318  0.000  ...  0.555  0.017      8      3     0.555\n",
              "1658  0.000  0.012  0.001  0.554  0.000  ...  0.423  0.009      3      9     0.554\n",
              "1765  0.000  0.005  0.000  0.395  0.000  ...  0.081  0.007      5      3     0.509\n",
              "1727  0.000  0.001  0.459  0.184  0.000  ...  0.352  0.000      2      3     0.459\n",
              "492   0.134  0.015  0.000  0.000  0.009  ...  0.442  0.000      8      6     0.442\n",
              "922   0.001  0.002  0.162  0.431  0.001  ...  0.048  0.001      3      7     0.431\n",
              "792   0.224  0.002  0.000  0.000  0.373  ...  0.010  0.001      4      6     0.373\n",
              "\n",
              "[10 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSGPr4g279Hx",
        "colab_type": "code",
        "outputId": "85d11ae1-68da-40c0-bbcf-7ecc485df518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "plot_image(df_err, \"Wrong Prediction\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADwCAYAAACjfbczAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF/znTHcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F17Gvez03R1",
        "colab_type": "code",
        "outputId": "440d97cd-afdf-4e02-a2ee-6b89e006d301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "prob_threshold = 0.995\n",
        "print(\"Correct prediction with high probability\")\n",
        "df_correct.loc[df[\"max_prob\"] >= prob_threshold]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction with high probability\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1058</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.995</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1618</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1768</th>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1759</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1332</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>433 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3    4  ...      8      9  ypred  ydata  max_prob\n",
              "1058  0.000  0.000  0.000  0.002  0.0  ...  0.000  0.995      9      9     0.995\n",
              "291   0.000  0.000  0.000  0.000  0.0  ...  0.000  0.005      5      5     0.995\n",
              "1126  0.000  0.995  0.000  0.000  0.0  ...  0.004  0.000      1      1     0.995\n",
              "1618  0.000  0.000  0.995  0.004  0.0  ...  0.000  0.000      2      2     0.995\n",
              "1768  0.995  0.000  0.000  0.000  0.0  ...  0.000  0.000      0      0     0.995\n",
              "...     ...    ...    ...    ...  ...  ...    ...    ...    ...    ...       ...\n",
              "603   0.000  0.000  0.000  0.000  1.0  ...  0.000  0.000      4      4     1.000\n",
              "1759  0.000  0.000  0.000  0.000  0.0  ...  0.000  1.000      9      9     1.000\n",
              "112   0.000  0.000  0.000  0.000  0.0  ...  0.000  0.000      7      7     1.000\n",
              "1600  0.000  0.000  1.000  0.000  0.0  ...  0.000  0.000      2      2     1.000\n",
              "1332  0.000  0.000  0.000  1.000  0.0  ...  0.000  0.000      3      3     1.000\n",
              "\n",
              "[433 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecrC9vsCEtgU",
        "colab_type": "code",
        "outputId": "949046db-68de-4a63-bbb1-9b8c6616abc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "print(\"Wrong prediction despite high probability\")\n",
        "df_err.loc[df[\"max_prob\"] >= prob_threshold]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction despite high probability\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>max_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ypred, ydata, max_prob]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I18CCJuooKDe",
        "colab_type": "text"
      },
      "source": [
        "# **Example 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqKoJScHXgwD",
        "colab_type": "text"
      },
      "source": [
        "### Configure Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkzUVo9mqtED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dnn_keras(num_features, num_labels, hiddens=[], dropout_rate=0.45, name=\"digits\"):\n",
        "  lname = \"%s_model\" % name\n",
        "  model = Sequential(name=lname)\n",
        "\n",
        "  is_hidden_layers = len(hiddens) > 0\n",
        "  num_node = num_labels\n",
        "  num_node = hiddens[0] if is_hidden_layers else num_labels\n",
        "  lname = \"1\" if is_hidden_layers else \"out\"\n",
        "  lname = \"%s_l%s\" % (name, lname)\n",
        "  model.add(Dense(num_node, input_dim=num_features, name=lname))\n",
        "\n",
        "  if (is_hidden_layers):\n",
        "    lname = \"%s_a1\" % name\n",
        "    model.add(Activation('relu', name=lname))\n",
        "    lname = \"%s_d1\" % name\n",
        "    model.add(Dropout(rate=dropout_rate, name=lname))\n",
        "    for layer_num, hidden_node_num in enumerate(hiddens[1:]):\n",
        "      lname = \"%s_l%i\" % (name, layer_num+2)\n",
        "      model.add(Dense(hidden_node_num, name=lname))\n",
        "      lname = \"%s_a%i\" % (name, layer_num+2)\n",
        "      model.add(Activation('relu', name=lname))\n",
        "      lname = \"%s_d%i\" % (name, layer_num+2)\n",
        "      model.add(Dropout(rate=dropout_rate, name=lname))\n",
        "    lname = \"%s_out\" % name\n",
        "    model.add(Dense(num_labels, name=lname))\n",
        "\n",
        "  lname = \"%s_aout\" % name\n",
        "  model.add(Activation('softmax', name=lname))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sJPDCZk0WKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_keras():\n",
        "  # load mnist dataset\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  y_data = y_test\n",
        "\n",
        "  # compute the number of labels\n",
        "  num_labels = len(np.unique(y_train))\n",
        "\n",
        "  # convert to one-hot vector\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  # image dimensions (assumed square)\n",
        "  image_size = x_train.shape[1]\n",
        "  input_size = image_size * image_size\n",
        "\n",
        "  # resize and normalize\n",
        "  x_train = np.reshape(x_train, [-1, input_size])\n",
        "  x_train = x_train.astype('float32') / 255\n",
        "  #x_train = np.apply_along_axis(lambda x: (x - x.mean() / x.std()), axis=1, arr=x_train) \n",
        "  x_test = np.reshape(x_test, [-1, input_size])\n",
        "  x_test = x_test.astype('float32') / 255\n",
        "  #x_test = np.apply_along_axis(lambda x: (x - x.mean() / x.std()), axis=1, arr=x_test)\n",
        "\n",
        "  return (x_train, x_test, y_train, y_test, y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H38jvPY2oOuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ex2():\n",
        "  is_trained = False\n",
        "\n",
        "  (x_train, x_test, y_train, y_test, y_data) = get_data_keras()\n",
        "  input_size = x_train.shape[1]\n",
        "  num_labels = y_train.shape[1]\n",
        "\n",
        "  # network parameters\n",
        "  batch_size = 128\n",
        "  hidden_layers = [256, 256]\n",
        "  dropout = 0.45\n",
        "  learning_rate = 0.01\n",
        "\n",
        "  # model is a 3-layer MLP with ReLU and dropout after each layer\n",
        "  model = build_dnn_keras(input_size, num_labels, hidden_layers, dropout)\n",
        "  model.summary()\n",
        "\n",
        "  # loss function for one-hot vector\n",
        "  # use of adam optimizer\n",
        "  # accuracy is a good metric for classification tasks\n",
        "  op = optimizers.Adam(lr=learning_rate)\n",
        "  model.compile(\n",
        "    loss=losses.categorical_crossentropy,\n",
        "    optimizer=op,\n",
        "    metrics=[metrics.categorical_accuracy])    # train the network\n",
        "\n",
        "\n",
        "  def _reset_weights(model):\n",
        "    K.get_session().close()\n",
        "    K.set_session(tf.Session())\n",
        "    K.get_session().run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "  def train(epochs=1, loss_delta=0.1):\n",
        "    nonlocal is_trained\n",
        "    if is_trained:\n",
        "      _reset_weights(model)\n",
        "\n",
        " \n",
        "    early_stopping_monitor = callbacks.EarlyStopping(\n",
        "      monitor='loss',\n",
        "      min_delta=loss_delta,\n",
        "      patience=2,\n",
        "      verbose=1,\n",
        "      restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(x_train, y_train, \n",
        "      epochs=epochs, \n",
        "      batch_size=batch_size,\n",
        "      callbacks=[early_stopping_monitor])\n",
        "    \n",
        "    is_trained = True\n",
        "    return history\n",
        "\n",
        "\n",
        "  def evaluate():\n",
        "    # validate the model on test dataset to determine generalization\n",
        "    print(\"Training metrics:\")\n",
        "    loss, accuracy = model.evaluate(x_train, y_train, batch_size=batch_size, verbose=0)\n",
        "    print({\"loss\": loss, \"accuracy\": accuracy})\n",
        "    print(\"Test metrics:\")\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "    print({\"loss\": loss, \"accuracy\": accuracy})\n",
        "\n",
        "\n",
        "  def predict():\n",
        "    #pred_class = model.predict_classes(x_test)\n",
        "    pred_prob = model.predict(x_test)\n",
        "    pred_class = np.argmax(pred_prob, axis=1)\n",
        "    df = pd.DataFrame(pred_prob.round(3), columns=range(10))\n",
        "    df[\"ypred\"] = pred_class\n",
        "    df[\"ydata\"] = y_data\n",
        "    df[\"prob\"] = df.iloc[:, 0:10].apply(lambda x: x.max(), axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "  return (train, evaluate, predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDnsLTKhXqv5",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTQRfgcf-Rq0",
        "colab_type": "code",
        "outputId": "b6cbf28d-579a-4027-e43d-68fa3792ff6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "train, evaluate, predict = ex2()"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"digits_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "digits_l1 (Dense)            (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "digits_a1 (Activation)       (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "digits_d1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "digits_l2 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "digits_a2 (Activation)       (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "digits_d2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "digits_out (Dense)           (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "digits_aout (Activation)     (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MrEeZy4CSZi",
        "colab_type": "code",
        "outputId": "1dd59f74-4ffc-4d12-8593-be942cd1a64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "hist = train(20, 0.01)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.4570 - categorical_accuracy: 0.8670\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.3743 - categorical_accuracy: 0.9011\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3589 - categorical_accuracy: 0.9092\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3513 - categorical_accuracy: 0.9140\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.3441 - categorical_accuracy: 0.9166\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.3531 - categorical_accuracy: 0.9196\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3319 - categorical_accuracy: 0.9219\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3316 - categorical_accuracy: 0.9229\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3400 - categorical_accuracy: 0.9230\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If0iYA5JB3jH",
        "colab_type": "code",
        "outputId": "fef91539-c3af-423e-c088-ea3fd85f0309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "evaluate()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training metrics:\n",
            "{'loss': 0.14508380025426545, 'accuracy': 0.9641166666666666}\n",
            "Test metrics:\n",
            "{'loss': 0.18203887531757354, 'accuracy': 0.957}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFHJU7ZjYFoK",
        "colab_type": "text"
      },
      "source": [
        "### Analyze Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAtDcS8u_LYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EaRQeAOH7Rt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "f079ed62-7fa0-49a2-c955-4c475190d67b"
      },
      "source": [
        "print(\"Correct prediction:\")\n",
        "df_correct = df.loc[df[\"ypred\"] == df[\"ydata\"]].sort_values([\"prob\"], axis=0, ascending=True)\n",
        "df_correct"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3225</th>\n",
              "      <td>0.055</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.148</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0.160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7678</th>\n",
              "      <td>0.057</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.106</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4874</th>\n",
              "      <td>0.110</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.170</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4140</th>\n",
              "      <td>0.057</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.113</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6625</th>\n",
              "      <td>0.067</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.122</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4067</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4066</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4064</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4061</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9570 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3      4  ...      8      9  ypred  ydata   prob\n",
              "3225  0.055  0.069  0.110  0.071  0.113  ...  0.144  0.148      7      7  0.160\n",
              "7678  0.057  0.081  0.170  0.111  0.070  ...  0.150  0.106      2      2  0.170\n",
              "4874  0.110  0.037  0.097  0.048  0.135  ...  0.147  0.170      9      9  0.170\n",
              "4140  0.057  0.060  0.128  0.084  0.099  ...  0.170  0.113      8      8  0.170\n",
              "6625  0.067  0.064  0.104  0.066  0.120  ...  0.182  0.122      8      8  0.182\n",
              "...     ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
              "4067  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      5      5  1.000\n",
              "4066  0.000  0.000  1.000  0.000  0.000  ...  0.000  0.000      2      2  1.000\n",
              "4064  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      7      7  1.000\n",
              "4061  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      7      7  1.000\n",
              "9999  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      6      6  1.000\n",
              "\n",
              "[9570 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7VVz4fMMKhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "bca1d3c9-9736-432f-80e9-25411311083f"
      },
      "source": [
        "print(\"Correct prediction probability spread:\")\n",
        "grp_correct = df_correct.groupby(pd.cut(df_correct.prob, np.linspace(0, 1, 21)))\n",
        "grp_correct.count().ypred"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction probability spread:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "prob\n",
              "(0.0, 0.05]       0\n",
              "(0.05, 0.1]       0\n",
              "(0.1, 0.15]       0\n",
              "(0.15, 0.2]      12\n",
              "(0.2, 0.25]      24\n",
              "(0.25, 0.3]      33\n",
              "(0.3, 0.35]      30\n",
              "(0.35, 0.4]      32\n",
              "(0.4, 0.45]      29\n",
              "(0.45, 0.5]      46\n",
              "(0.5, 0.55]      48\n",
              "(0.55, 0.6]      59\n",
              "(0.6, 0.65]      55\n",
              "(0.65, 0.7]      65\n",
              "(0.7, 0.75]      63\n",
              "(0.75, 0.8]      81\n",
              "(0.8, 0.85]      85\n",
              "(0.85, 0.9]     137\n",
              "(0.9, 0.95]     226\n",
              "(0.95, 1.0]    8545\n",
              "Name: ypred, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md1sVlgmRsnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "2484f6ee-2a99-48e1-c5d3-40bec4ed4c88"
      },
      "source": [
        "print(\"Wrong prediction:\")\n",
        "df_err = df.loc[df[\"ypred\"] != df[\"ydata\"]].sort_values([\"prob\"], axis=0, ascending=False)\n",
        "df_err"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>ypred</th>\n",
              "      <th>ydata</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2654</th>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1260</th>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4211</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1014</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6568</th>\n",
              "      <td>0.066</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.112</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>0.169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444</th>\n",
              "      <td>0.106</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.149</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>0.165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1039</th>\n",
              "      <td>0.053</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.150</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0.164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2919</th>\n",
              "      <td>0.041</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.151</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9342</th>\n",
              "      <td>0.109</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.097</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>430 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3      4  ...      8      9  ypred  ydata   prob\n",
              "2654  0.000  1.000  0.000  0.000  0.000  ...  0.000  0.000      1      6  1.000\n",
              "1260  0.000  1.000  0.000  0.000  0.000  ...  0.000  0.000      1      7  1.000\n",
              "4211  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      5      6  1.000\n",
              "965   1.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      0      6  1.000\n",
              "1014  0.000  0.000  0.000  0.000  0.000  ...  0.000  0.000      5      6  1.000\n",
              "...     ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
              "6568  0.066  0.077  0.116  0.067  0.169  ...  0.148  0.112      4      9  0.169\n",
              "1444  0.106  0.036  0.103  0.023  0.151  ...  0.097  0.149      7      6  0.165\n",
              "1039  0.053  0.075  0.111  0.078  0.094  ...  0.164  0.150      8      7  0.164\n",
              "2919  0.041  0.050  0.092  0.067  0.162  ...  0.137  0.151      4      5  0.162\n",
              "9342  0.109  0.156  0.151  0.110  0.039  ...  0.113  0.097      1      3  0.156\n",
              "\n",
              "[430 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUk9zGDWSG9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "772d838a-a337-4485-82e5-de87cb82e6e3"
      },
      "source": [
        "print(\"Wrong prediction probability spread:\")\n",
        "grp_err = df_err.groupby(pd.cut(df_err.prob, np.linspace(0, 1, 21)))\n",
        "grp_err.count().ypred"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction probability spread:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "prob\n",
              "(0.0, 0.05]     0\n",
              "(0.05, 0.1]     0\n",
              "(0.1, 0.15]     0\n",
              "(0.15, 0.2]    42\n",
              "(0.2, 0.25]    42\n",
              "(0.25, 0.3]    35\n",
              "(0.3, 0.35]    30\n",
              "(0.35, 0.4]    28\n",
              "(0.4, 0.45]    19\n",
              "(0.45, 0.5]    23\n",
              "(0.5, 0.55]    21\n",
              "(0.55, 0.6]    31\n",
              "(0.6, 0.65]    14\n",
              "(0.65, 0.7]    17\n",
              "(0.7, 0.75]     8\n",
              "(0.75, 0.8]    15\n",
              "(0.8, 0.85]    14\n",
              "(0.85, 0.9]    15\n",
              "(0.9, 0.95]    16\n",
              "(0.95, 1.0]    60\n",
              "Name: ypred, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    }
  ]
}